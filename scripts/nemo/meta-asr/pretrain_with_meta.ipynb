{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from pytorch_lightning import Trainer\n",
    "from nemo.collections.asr.models import ASRModel\n",
    "from nemo.collections.common.parts.adapter_modules import LinearAdapterConfig\n",
    "from nemo.utils import model_utils\n",
    "from nemo.core import adapter_mixins\n",
    "import sentencepiece as spm\n",
    "from nemo.utils import exp_manager\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"  # Specify the GPUs you want to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRModelTrainer:\n",
    "    def __init__(self, config_path):\n",
    "        self.load_config(config_path)\n",
    "        self.cfg = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def load_config(self, config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        self.model_root = Path(self.config['model']['model_root'])\n",
    "        self.model_path = self.model_root / self.config['model']['model_name']\n",
    "        self.tokenizer_dir = self.model_root / self.config['model']['tokenizer_folder']\n",
    "        self.extended_tokenizer_dir = self.model_root / self.config['model']['new_tokenizer_folder']\n",
    "        self.tokenizer_model_file = self.tokenizer_dir / 'tokenizer.model'\n",
    "        self.vocab_file = self.tokenizer_dir / 'tokenizer.vocab'\n",
    "        self.vocab_txt_file = self.tokenizer_dir / 'vocab.txt'\n",
    "        self.proto_file = self.config['model']['proto_file']\n",
    "        self.proto_dir = self.config['model']['proto_dir']\n",
    "        self.data_dir = Path(self.config['training']['data_dir'])\n",
    "        self.train_manifest = self.data_dir / self.config['training']['train_manifest']\n",
    "        self.test_manifest = self.data_dir / self.config['training']['test_manifest']\n",
    "        self.batch_size = self.config['training']['batch_size']\n",
    "        self.max_steps = self.config['training']['max_steps']\n",
    "        self.exp_config = exp_manager.ExpManagerConfig(\n",
    "            exp_dir=self.config['experiment']['exp_dir'],\n",
    "            name=self.config['experiment']['exp_name'],\n",
    "            checkpoint_callback_params=exp_manager.CallbackParams(\n",
    "                monitor=self.config['experiment']['monitor'],\n",
    "                mode=self.config['experiment']['mode'],\n",
    "                always_save_nemo=self.config['experiment']['always_save_nemo'],\n",
    "                save_best_model=self.config['experiment']['save_best_model'],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def load_and_update_model_config(self):\n",
    "        self.cfg = ASRModel.restore_from(restore_path=self.model_path, return_config=True)\n",
    "        self.cfg = self.update_model_config_to_support_adapter(self.cfg)\n",
    "        print(self.cfg)\n",
    "    \n",
    "    def restore_model_with_updated_config(self):\n",
    "        self.model = ASRModel.restore_from(self.model_path, override_config_path=self.cfg)\n",
    "    \n",
    "    def prepare_data_and_tokens_with_meta(self, tokenizer_state=\"extended\", meta_token_count=3000):\n",
    "        \n",
    "        meta_tokens = [f\"META{i}\" for i in range(meta_token_count)]\n",
    "        \n",
    "        punctuations = ['.', ',', '?', '!', ';', ':', '-', '(', ')', '[', ']', '{', '}', '<', '>', '/', '\\\\', '|', '@', '#', '$', '%', '^', '&', '*', '+', '=', '~', '`', '_', '\"', \"'\"]\n",
    "        index_tokens = [str(i) for i in range(10)]\n",
    "        tokens = meta_tokens + index_tokens + punctuations\n",
    "        \n",
    "        self.edit_spt_model(self.tokenizer_model_file, self.extended_tokenizer_dir, tokens, self.vocab_file, self.vocab_txt_file, is_userdefined)\n",
    "        self.model.change_vocabulary(self.extended_tokenizer_dir, \"bpe\")\n",
    "    \n",
    "    def prepare_data_and_tokens(self, tags_type=\"mapped\", tokenizer_state=\"extended\", vocab_size=2000):\n",
    "        taglist = []\n",
    "        \n",
    "        if tags_type == \"mapped\":\n",
    "            ### here we assume two columns tags file: word \\t tag\n",
    "            \n",
    "            all_tags_path = os.path.join(self.data_dir, \"filtered_alltags.txt\")\n",
    "            with open(all_tags_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    word, tag = line.split()\n",
    "                    taglist.append(tag)\n",
    "        \n",
    "        else:\n",
    "            all_tags_path = os.path.join(self.data_dir, \"keywords.txt\")\n",
    "            tagdata = open(all_tags_path, 'r').read()\n",
    "            # Filter out empty lines\n",
    "            for line in tagdata.split('\\n'):\n",
    "                if line.strip():\n",
    "                    taglist.append(line)\n",
    "            \n",
    "            ### just a json readable list of tags\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        punctuations = ['.', ',', '?', '!', ';', ':', '-', '(', ')', '[', ']', '{', '}', '<', '>', '/', '\\\\', '|', '@', '#', '$', '%', '^', '&', '*', '+', '=', '~', '`', '_', '\"', \"'\"]\n",
    "        index_tokens = [str(i) for i in range(10)]\n",
    "        tokens = taglist + index_tokens + punctuations\n",
    "        is_userdefined = True\n",
    "\n",
    "        if tokenizer_state == \"new\":\n",
    "            _ = train_sentencepiece_tokenizer(self.train_manifest, self.extended_tokenizer_dir, special_tokens=taglist, vocab_size=vocab_size)\n",
    "            self.model.change_vocabulary(self.extended_tokenizer_dir, \"bpe\")\n",
    "        \n",
    "        elif tokenizer_state == \"extended\":\n",
    "            self.edit_spt_model(self.tokenizer_model_file, self.extended_tokenizer_dir, tokens, self.vocab_file, self.vocab_txt_file, is_userdefined)\n",
    "            self.model.change_vocabulary(self.extended_tokenizer_dir, \"bpe\")\n",
    "        \n",
    "        else:\n",
    "            print(\"Using the existing tokenizer model and vocab files\")\n",
    "    \n",
    "    def configure_trainer(self):\n",
    "        accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            devices=[0, 1], \n",
    "            accelerator=accelerator, \n",
    "            max_steps=self.max_steps,\n",
    "            enable_checkpointing=False, \n",
    "            logger=False,\n",
    "            log_every_n_steps=50, \n",
    "            check_val_every_n_epoch=1,\n",
    "            accumulate_grad_batches=4\n",
    "        )\n",
    "\n",
    "        self.model.set_trainer(self.trainer)\n",
    "    \n",
    "    def configure_model_for_training(self):\n",
    "        with open_dict(self.model.cfg):\n",
    "            self.model.cfg.train_ds.manifest_filepath = str(self.train_manifest)\n",
    "            self.model.cfg.train_ds.batch_size = self.batch_size\n",
    "            self.model.cfg.train_ds.is_tarred = False\n",
    "            self.model.cfg.train_ds.tarred_audio_filepaths = None\n",
    "            self.model.cfg.validation_ds.manifest_filepath = str(self.test_manifest)\n",
    "            self.model.cfg.validation_ds.batch_size = self.batch_size\n",
    "            self.model.cfg.train_ds.num_workers = 8  # Adding num_workers for training dataloader\n",
    "\n",
    "        self.model.setup_training_data(self.model.cfg.train_ds)\n",
    "        self.model.setup_multiple_validation_data(self.model.cfg.validation_ds)\n",
    "        self.model.setup_multiple_test_data(self.model.cfg.validation_ds)\n",
    "        \n",
    "\n",
    "\n",
    "    def configure_spec_augmentation(self):\n",
    "        with open_dict(self.model.cfg):\n",
    "            self.model.cfg.spec_augment.freq_masks = self.model.cfg.spec_augment.freq_masks\n",
    "            self.model.cfg.spec_augment.freq_width = self.model.cfg.spec_augment.freq_width\n",
    "            self.model.cfg.spec_augment.time_masks = self.model.cfg.spec_augment.time_masks\n",
    "            self.model.cfg.spec_augment.time_width = self.model.cfg.spec_augment.time_width\n",
    "\n",
    "        self.model.spec_augmentation = self.model.from_config_dict(self.model.cfg.spec_augment)\n",
    "\n",
    "    def configure_optimization(self):\n",
    "        if 'optim' in self.model.cfg:\n",
    "            print(OmegaConf.to_yaml(self.model.cfg.optim))\n",
    "\n",
    "        with open_dict(self.model.cfg):\n",
    "            self.model.cfg.optim.lr = 0.1\n",
    "            self.model.cfg.optim.weight_decay = 0.001\n",
    "            self.model.cfg.optim.sched.warmup_steps = 1000\n",
    "\n",
    "        self.model.setup_optimization(self.model.cfg.optim)\n",
    "    \n",
    "    def setup_adapters(self):\n",
    "        if hasattr(self.model, 'adapter_module_names'):\n",
    "            print(self.model.adapter_module_names)\n",
    "\n",
    "        for module in self.model.children():\n",
    "            if hasattr(module, 'get_accepted_adapter_types'):\n",
    "                types = module.get_accepted_adapter_types()\n",
    "                print(\"Module:\", module.__class__.__name__)\n",
    "                for tp in types:\n",
    "                    print(tp)\n",
    "                print()\n",
    "\n",
    "        for adapter_name, adapter_config in self.config['adapters'].items():\n",
    "            adapter_cfg = LinearAdapterConfig(\n",
    "                in_features=self.model.cfg.encoder.d_model,  # Set in_features based on model configuration\n",
    "                dim=adapter_config['dim'],\n",
    "                activation=adapter_config['activation'],\n",
    "                norm_position=adapter_config['norm_position'],\n",
    "            )\n",
    "            print(f\"Adding adapter {adapter_name} with config: {adapter_cfg}\")\n",
    "\n",
    "            self.model.add_adapter(name=adapter_config['name'], cfg=adapter_cfg)\n",
    "\n",
    "        self.model.set_enabled_adapters(enabled=False)  # Disable all adapters\n",
    "\n",
    "        # Enable only the adapters specified in the config\n",
    "        for adapter_name, adapter_config in self.config['adapters'].items():\n",
    "            self.model.set_enabled_adapters(name=adapter_config['name'], enabled=True)\n",
    "\n",
    "        #self.model.freeze()\n",
    "        self.model.unfreeze_enabled_adapters()\n",
    "        self.model.decoder.unfreeze()\n",
    "        self.model.summarize()\n",
    "\n",
    "    def prepare_experiment_manager(self):\n",
    "        # Environment variable generally used for multi-node multi-gpu training.\n",
    "        # In notebook environments, this flag is unnecessary and can cause logs of multiple training runs to overwrite each other.\n",
    "        os.environ.pop('NEMO_EXPM_VERSION', None)\n",
    "\n",
    "        exp_config = OmegaConf.structured(self.exp_config)\n",
    "\n",
    "        logdir = exp_manager.exp_manager(self.trainer, exp_config)\n",
    "        print(f\"Experiment log directory: {logdir}\")\n",
    "\n",
    "    def train(self):\n",
    "        self.trainer.fit(self.model)\n",
    "\n",
    "    def summarize_model(self):\n",
    "        self.model.summarize()\n",
    "\n",
    "    @staticmethod\n",
    "    def update_model_config_to_support_adapter(model_cfg):\n",
    "        with open_dict(model_cfg):\n",
    "            adapter_metadata = adapter_mixins.get_registered_adapter(model_cfg.encoder._target_)\n",
    "            if adapter_metadata is not None:\n",
    "                model_cfg.encoder._target_ = adapter_metadata.adapter_class_path\n",
    "        print(\"Updated encoder _target_ model:\", model_cfg.encoder._target_)\n",
    "        return model_cfg\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_sentencepiece_model_pb2(script_dir, proto_file_path):\n",
    "        command = ['protoc', f'--python_out={script_dir}', proto_file_path]\n",
    "        try:\n",
    "            subprocess.run(command, check=True)\n",
    "            print(\"Successfully generated sentencepiece_model_pb2.py\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error generating sentencepiece_model_pb2.py: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def edit_spt_model(tokenizer_model_file, extended_tokenizer_dir, tokens, vocab_file, vocab_txt_file, is_userdefined=False):\n",
    "        if not os.path.exists(extended_tokenizer_dir):\n",
    "            os.makedirs(extended_tokenizer_dir)\n",
    "        \n",
    "        output_model_file = os.path.join(extended_tokenizer_dir, 'tokenizer.model')\n",
    "        output_vocab_file = os.path.join(extended_tokenizer_dir, 'tokenizer.vocab')\n",
    "        output_vocab_txt_file = os.path.join(extended_tokenizer_dir, 'vocab.txt')\n",
    "        token_type = 3 if not is_userdefined else 4\n",
    "        \n",
    "        from sentencepiece import sentencepiece_model_pb2 as sp_pb2\n",
    "\n",
    "        model = sp_pb2.ModelProto()\n",
    "        model.ParseFromString(open(tokenizer_model_file, 'rb').read())\n",
    "        existing_tokens = {piece.piece for piece in model.pieces}\n",
    "        new_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Skip empty or whitespace-only tokens\n",
    "            if not token.strip():\n",
    "                logging.warning(\"Skipping empty token.\")\n",
    "                continue\n",
    "            if token in existing_tokens:\n",
    "                logging.warning(f\"Special Token '{token}' already exists in the input model, skipping.\")\n",
    "                continue\n",
    "            piece = model.SentencePiece(piece=token, score=0.0, type=token_type)\n",
    "            model.pieces.append(piece)\n",
    "            new_tokens.append(token)\n",
    "\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        #try:\n",
    "        sp.LoadFromSerializedProto(model.SerializeToString())\n",
    "        for token in new_tokens:\n",
    "            id = sp.piece_to_id(token)\n",
    "            print(\"Token: \", token, \"ID: \", id)\n",
    "            logging.info(f\"Created token '{token}' at ID {id}\")\n",
    "        logging.info(f\"New tokenizer vocab size: {sp.get_piece_size()}\")\n",
    "        # except:\n",
    "        #     logging.error(\"Could not appropriately configure new tokenizer. Verify if the special tokens already exist.\")\n",
    "        #     sys.exit(1)\n",
    "\n",
    "        with open(output_model_file, 'wb') as outf:\n",
    "            outf.write(model.SerializeToString())\n",
    "        logging.info(f\"Created new tokenizer at: {output_model_file}\")\n",
    "\n",
    "        # Read the original vocab file and append the new tokens\n",
    "        with open(vocab_file, 'r') as original_vocab_file:\n",
    "            original_vocab = original_vocab_file.readlines()\n",
    "        with open(output_vocab_file, 'w') as updated_vocab_file:\n",
    "            updated_vocab_file.writelines(original_vocab)\n",
    "            for token in new_tokens:\n",
    "                updated_vocab_file.write(f\"{token}\\n\")\n",
    "\n",
    "        # Update vocab.txt\n",
    "        with open(vocab_txt_file, 'r') as original_vocab_txt_file:\n",
    "            original_vocab_txt = original_vocab_txt_file.readlines()\n",
    "        with open(output_vocab_txt_file, 'w') as updated_vocab_txt_file:\n",
    "            updated_vocab_txt_file.writelines(original_vocab_txt)\n",
    "            for token in new_tokens:\n",
    "                updated_vocab_txt_file.write(f\"{token}\\n\")\n",
    "\n",
    "        logging.info(f\"Updated vocab files: {output_vocab_file}, {output_vocab_txt_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ASRModelTrainer(config_path='config/config.yml')\n",
    "model_trainer.load_and_update_model_config()\n",
    "model_trainer.restore_model_with_updated_config()\n",
    "model_trainer.prepare_data_and_tokens(tags_type=\"unmapped\", tokenizer_state=\"extended\", vocab_size=1704)\n",
    "model_trainer.configure_trainer()\n",
    "model_trainer.configure_model_for_training()\n",
    "model_trainer.configure_spec_augmentation()\n",
    "model_trainer.configure_optimization()\n",
    "#model_trainer.setup_adapters()\n",
    "model_trainer.prepare_experiment_manager()\n",
    "model_trainer.summarize_model()\n",
    "model_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
