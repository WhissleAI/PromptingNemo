{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksingla/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "\n",
      "***** Exporting submodel 1/1: RobertaForSequenceClassification *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/external/ksingla/artifacts/model_shelf/zero-shot-classify-SSTuning-base/tokenizer_config.json',\n",
       " '/external/ksingla/artifacts/model_shelf/zero-shot-classify-SSTuning-base/special_tokens_map.json',\n",
       " '/external/ksingla/artifacts/model_shelf/zero-shot-classify-SSTuning-base/vocab.json',\n",
       " '/external/ksingla/artifacts/model_shelf/zero-shot-classify-SSTuning-base/merges.txt',\n",
       " '/external/ksingla/artifacts/model_shelf/zero-shot-classify-SSTuning-base/added_tokens.json',\n",
       " '/external/ksingla/artifacts/model_shelf/zero-shot-classify-SSTuning-base/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_checkpoint = \"DAMO-NLP-SG/zero-shot-classify-SSTuning-base\"\n",
    "model_shelf =  \"/external/ksingla/artifacts/model_shelf\"\n",
    "save_directory = Path(model_shelf) / \"zero-shot-classify-SSTuning-base\"\n",
    "\n",
    "# Load a model from transformers and export it to ONNX\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Save the onnx model and tokenizer\n",
    "ort_model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"task\": \"text_classification_zeroshot\",\n",
    "    \"hf_id\": \"DAMO-NLP-SG/zero-shot-classify-SSTuning-base\",\n",
    "    \"sample_rate\": 16000,\n",
    "    \"encoder.onnx\": \"model.onnx\",\n",
    "    \"tokenizer.model\": \"tokenizer/tokenizer.model\",\n",
    "    \"onnx.intra_op_num_threads\": 1\n",
    "}\n",
    "\n",
    "# Convert dictionary to plain text format\n",
    "config_text = \"\\n\".join(f\"{key}={value}\" for key, value in config.items()) + \"\\n\"\n",
    "\n",
    "# Write the plain text to magic.txt\n",
    "magic_file = open(save_directory / \"magic.txt\",'w')\n",
    "magic_file.write(config_text)\n",
    "magic_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "class ONNXTextClassifier:\n",
    "    def __init__(self, model_path, tokenizer_path, device=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.onnx_session = InferenceSession(str(model_path))\n",
    "        self.device = device if device else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.list_ABC = [x for x in string.ascii_uppercase]\n",
    "\n",
    "    def prepare_text(self, text, list_label, shuffle=False):\n",
    "        list_label = [x + '.' if x[-1] != '.' else x for x in list_label]\n",
    "        list_label_new = list_label + [self.tokenizer.pad_token] * (20 - len(list_label))\n",
    "        if shuffle:\n",
    "            random.shuffle(list_label_new)\n",
    "        s_option = ' '.join(['(' + self.list_ABC[i] + ') ' + list_label_new[i] for i in range(len(list_label_new))])\n",
    "        formatted_text = f'{s_option} {self.tokenizer.sep_token} {text}'\n",
    "        encoding = self.tokenizer([formatted_text], truncation=True, max_length=512, return_tensors='pt')\n",
    "        return encoding\n",
    "\n",
    "    def check_text(self, text, list_label, shuffle=False):\n",
    "        encoding = self.prepare_text(text, list_label, shuffle)\n",
    "        input_ids = encoding['input_ids'].cpu().numpy()\n",
    "        attention_mask = encoding['attention_mask'].cpu().numpy()\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "        outputs = self.onnx_session.run(['logits'], inputs)\n",
    "        logits = torch.tensor(outputs[0])\n",
    "\n",
    "        logits = logits if shuffle else logits[:, 0:len(list_label)]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1).tolist()\n",
    "        predictions = torch.argmax(logits, dim=-1).item()\n",
    "        probabilities = [round(x, 5) for x in probs[0]]\n",
    "\n",
    "        print(f'prediction:    {predictions} => ({self.list_ABC[predictions]}) {list_label[predictions]}')\n",
    "        print(f'probability:   {round(probabilities[predictions] * 100, 2)}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:    1 => (B) positive\n",
      "probability:   99.92%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model_shelf =  \"/external/ksingla/artifacts/model_shelf\"\n",
    "save_directory = Path(model_shelf) / \"zero-shot-classify-SSTuning-base\"\n",
    "onnx_model_path = save_directory / \"model.onnx\"\n",
    "tokenizer_path = \"DAMO-NLP-SG/zero-shot-classify-SSTuning-base\"\n",
    "\n",
    "classifier = TextClassifier(model_path=onnx_model_path, tokenizer_path=tokenizer_path)\n",
    "\n",
    "text = \"I love this place! The food is always so fresh and delicious.\"\n",
    "list_label = [\"negative\", \"positive\"]\n",
    "classifier.check_text(text, list_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
