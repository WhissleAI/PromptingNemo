{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing English LibreSpeech Corpus into Manfests for Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from pathlib import PurePath\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "from nemo.collections import nlp as nemo_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mp3_to_wav(mp3_file_path, wav_file_path):\n",
    "    # Load the MP3 file\n",
    "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
    "\n",
    "    # Export as WAV\n",
    "    audio.export(wav_file_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-14 08:46:38 cloud:58] Found existing object /home/ubuntu/.cache/torch/NeMo/NeMo_1.21.0rc0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo.\n",
      "[NeMo I 2023-11-14 08:46:38 cloud:64] Re-using file from: /home/ubuntu/.cache/torch/NeMo/NeMo_1.21.0rc0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo\n",
      "[NeMo I 2023-11-14 08:46:38 common:913] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2023-11-14 08:46:41 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: distilbert-base-uncased, vocab_file: /tmp/tmp9zbd0rcj/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-14 08:46:41 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2023-11-14 08:46:41 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-14 08:46:41 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-14 08:46:41 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-14 08:46:41 nlp_overrides:438] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
      "    Megatron-based models require Apex to function correctly.\n",
      "[NeMo W 2023-11-14 08:46:41 nlp_overrides:446] megatron-core was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/NeMo#megatron-gpt.\n",
      "[NeMo W 2023-11-14 08:46:43 punctuation_capitalization_model:719] The artifact `class_labels.punct_labels_file` was not found in checkpoint. Will rely on `punct_label_ids` parameter\n",
      "[NeMo W 2023-11-14 08:46:43 punctuation_capitalization_model:741] The artifact `class_labels.capit_labels_file` was not found in checkpoint. Will rely on `capit_label_ids` parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-14 08:46:44 save_restore_connector:249] Model PunctuationCapitalizationModel was successfully restored from /home/ubuntu/.cache/torch/NeMo/NeMo_1.21.0rc0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo.\n"
     ]
    }
   ],
   "source": [
    "### Intitiate text normalizer and puctuator\n",
    "normalizer = Normalizer(input_case='lower_cased', lang=\"en\")\n",
    "punctuator = nemo_nlp.models.PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_distilbert\")\n",
    "\n",
    "def normalize(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    normalized = normalizer.normalize(text, verbose=True, punct_post_process=True)\n",
    "    normalized = [normalized]\n",
    "    norm_punctuated = punctuator.add_punctuation_capitalization(normalized)[0]\n",
    "    return norm_punctuated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/disk1/audio_datasets/CommonVoice/datasets/cv-corpus-15.0-2023-09-08/en/clips-wav\n"
     ]
    }
   ],
   "source": [
    "### Define all data path (SLURP here)\n",
    "cv_english = PurePath(\"/n/disk1/audio_datasets/CommonVoice/datasets/cv-corpus-15.0-2023-09-08/en/\")\n",
    "train_annotations = cv_english / PurePath(\"train.tsv\")\n",
    "dev_annotations = cv_english / PurePath(\"dev.tsv\")\n",
    "test_annotations = cv_english / PurePath(\"test.tsv\")\n",
    "\n",
    "audioclips = PurePath(\"/n/disk1/audio_datasets/CommonVoice/datasets/cv-corpus-15.0-2023-09-08/en/clips\")\n",
    "audioclipswav = PurePath(str(audioclips) + \"-wav\")\n",
    "os.system(\"mkdir -p \" + str(audioclipswav))\n",
    "print(audioclipswav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-14 09:16:49 nemo_logging:349] /home/ubuntu/anaconda3/envs/nemo/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "entity_tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "entity_model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "hf_nlp = pipeline(\"ner\", model=entity_model, tokenizer=entity_tokenizer, grouped_entities=True)\n",
    "\n",
    "\n",
    "def tag_entities(text):\n",
    "\n",
    "    ner_results = hf_nlp(text)\n",
    "    print(ner_results)\n",
    "\n",
    "    # example: [{'entity_group': 'PER', 'score': 0.8913538, 'word': 'Min', 'start': 0, 'end': 3}, {'entity_group': 'LOC', 'score': 0.9983326, 'word': 'West Van Buren Street', 'start': 93, 'end': 114}]\n",
    "    for ner_dict in ner_results:\n",
    "\n",
    "        entity_group = ner_dict['entity_group']\n",
    "        start = ner_dict['start']\n",
    "        end = ner_dict['end']\n",
    "        word = ner_dict['word']\n",
    "\n",
    "        text = text.replace(word, \"B-\"+entity_group+\" \"+word+\" E-\"+entity_group)\n",
    "\n",
    "    print(\"ner tagged text\", text)\n",
    "\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rajaram1996/Hubert_emotion were not used when initializing HubertForSpeechClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSpeechClassification were not initialized from the model checkpoint at Rajaram1996/Hubert_emotion and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### Start pretrained Emotion Classification system\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor\n",
    "from AudioEmotionClassification.models import Wav2Vec2ForSpeechClassification, HubertForSpeechClassification\n",
    "\n",
    "emotion_model = HubertForSpeechClassification.from_pretrained(\"Rajaram1996/Hubert_emotion\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "sampling_rate=16000 # defined by the model; must convert mp3 to this rate.\n",
    "config = AutoConfig.from_pretrained(\"Rajaram1996/Hubert_emotion\")\n",
    "\n",
    "def speech_file_to_array_fn(path, sampling_rate):\n",
    "    speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(_sampling_rate, sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "def predict(path, sampling_rate):\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in\n",
    "               enumerate(scores)]\n",
    "    return outputs\n",
    "\n",
    "def get_emotion_labels(audio_file, sampling_rate=16000, score=50.0):\n",
    "    sound_array = speech_file_to_array_fn(audio_file, sampling_rate)\n",
    "    \n",
    "    inputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(\"cpu\").float() for key in inputs}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "    outputs = [{\n",
    "        \"emo\": config.id2label[i],\n",
    "        \"score\": round(score * 100, 1)}\n",
    "        for i, score in enumerate(scores)\n",
    "    ]\n",
    "\n",
    "    #[{'emo': 'female_neutral', 'score': 73.9}, {'emo': 'female_happy', 'score': 24.8}]\n",
    "    emotion_labels = [row for row in sorted(outputs, key=lambda x:x[\"score\"], reverse=True) if row['score'] != '0.0%'][:2]\n",
    "\n",
    "    all_labels = []\n",
    "    for emotion_dict in emotion_labels:\n",
    "        label = emotion_dict['emo'].split(\"_\")[1].upper()\n",
    "        score = emotion_dict['score']\n",
    "\n",
    "        if score > 50.0:\n",
    "            all_labels.append(label)\n",
    "\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tsv(tsvfile, audioclips, audioclipswav, manifestfile):\n",
    "    \n",
    "    tsvfile = pd.read_csv(tsvfile, sep=\"\\t\")\n",
    "    print(manifestfile)\n",
    "    manifest = open(str(manifestfile),'w')\n",
    "    #data_top = tsvfile.columns.values\n",
    "\n",
    "    #print(data_top)\n",
    "    for index, row in tsvfile.iterrows():\n",
    "        audiofile = audioclips / row['path']\n",
    "        audiofilewav = audioclipswav / PurePath(row['path'].split(\".\")[0]+\".wav\")\n",
    "        \n",
    "        convert_mp3_to_wav(audiofile, audiofilewav)\n",
    "        \n",
    "        text = row['sentence']\n",
    "        text_tagged = tag_entities(text)\n",
    "        emotion_labels = get_emotion_labels(audio_file=audiofilewav, sampling_rate=16000)\n",
    "        text_tagged_emotion = text_tagged + \" \" + \" \".join(emotion_labels)\n",
    "\n",
    "        sample_dict = {}\n",
    "        sample_dict['audiofilepath'] = str(audiofilewav)\n",
    "        sample_dict['text'] = text\n",
    "        sample_dict['tagged_text'] = text\n",
    "        sample_dict['instruction'] = \"transcribe speech\"\n",
    "        print(sample_dict)\n",
    "        json.dump(sample_dict, manifest)\n",
    "        manifest.write(\"\\n\")\n",
    "\n",
    "        sample_dict['tagged_text'] = text_tagged\n",
    "        sample_dict['instruction'] = \"transcribe and mark named entities\"\n",
    "        json.dump(sample_dict, manifest)\n",
    "        manifest.write(\"\\n\")\n",
    "\n",
    "        sample_dict['tagged_text'] = text_tagged_emotion\n",
    "        sample_dict['instruction'] = \"transcribe, mark named entitites and track speaker emotion\"\n",
    "        json.dump(sample_dict, manifest)\n",
    "        manifest.write(\"\\n\")\n",
    "           \n",
    "        print(text_tagged, audiofilewav)\n",
    "    \n",
    "    manifest.close()\n",
    "\n",
    "    #for line in tsvfile:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/disk1/audio_datasets/manifests/dev_cv_en.json\n",
      "[]\n",
      "ner tagged text Because of facial deformity, she lives a life of fear and shame.\n",
      "{'audiofilepath': PurePosixPath('/n/disk1/audio_datasets/CommonVoice/datasets/cv-corpus-15.0-2023-09-08/en/clips-wav/common_voice_en_19624951.wav'), 'text': 'Because of facial deformity, she lives a life of fear and shame.', 'tagged_text': 'Because of facial deformity, she lives a life of fear and shame.', 'instruction': 'transcribe speech'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type PurePosixPath is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/n/disk1/PromptingNemo/process_cv.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m manifestfolder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/n/disk1/audio_datasets/manifests\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m process_tsv(tsvfile\u001b[39m=\u001b[39;49mdev_annotations, audioclips\u001b[39m=\u001b[39;49maudioclips, audioclipswav\u001b[39m=\u001b[39;49maudioclipswav, manifestfile\u001b[39m=\u001b[39;49mmanifestfolder\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/dev_cv_en.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m process_tsv(tsvfile\u001b[39m=\u001b[39mtrain_annotations, audioclips\u001b[39m=\u001b[39maudioclips, audioclipswav\u001b[39m=\u001b[39maudioclipswav, manifestfile\u001b[39m=\u001b[39mmanifestfolder\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/train_cv_en.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m process_tsv(tsvfile\u001b[39m=\u001b[39mtest_annotations, audioclips\u001b[39m=\u001b[39maudioclips, audioclipswav\u001b[39m=\u001b[39maudioclipswav, manifestfile\u001b[39m=\u001b[39mmanifestfolder\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/test_cv_en.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/n/disk1/PromptingNemo/process_cv.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m sample_dict[\u001b[39m'\u001b[39m\u001b[39minstruction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtranscribe speech\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(sample_dict)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m json\u001b[39m.\u001b[39;49mdump(sample_dict, manifest)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m manifest\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_cv.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m sample_dict[\u001b[39m'\u001b[39m\u001b[39mtagged_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m text_tagged\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[39m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[1;32m    439\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type PurePosixPath is not JSON serializable"
     ]
    }
   ],
   "source": [
    "manifestfolder = \"/n/disk1/audio_datasets/manifests\"\n",
    "process_tsv(tsvfile=dev_annotations, audioclips=audioclips, audioclipswav=audioclipswav, manifestfile=manifestfolder+\"/dev_cv_en.json\")\n",
    "process_tsv(tsvfile=train_annotations, audioclips=audioclips, audioclipswav=audioclipswav, manifestfile=manifestfolder+\"/train_cv_en.json\")\n",
    "process_tsv(tsvfile=test_annotations, audioclips=audioclips, audioclipswav=audioclipswav, manifestfile=manifestfolder+\"/test_cv_en.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
