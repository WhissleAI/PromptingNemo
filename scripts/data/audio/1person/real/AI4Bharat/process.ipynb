{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"/external1/hkoduri/data/malayalam_processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to a specific folder in your Google Drive\n",
    "os.chdir('/external1/hkoduri/data/malayalam_processed')\n",
    "\n",
    "# Verify the current directory\n",
    "print(\"Current Directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://indicvoices.ai4bharat.org/backend/download_dataset/v2_Malayalam_valid.tgz?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3MzM5Mjg0NDYsImlhdCI6MTczMzc1NTY0NiwiZW1haWwiOiJoa29kdXJpQHdoaXNzbGUuYWkifQ.E8xgZy_Rmpub0Q6P7YvvCJDtIC0_LxCApA0H-oIWLZQ -O v2_Malayalam_valid.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://indicvoices.ai4bharat.org/backend/download_dataset/v2_Malayalam_train.tgz?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3MzM5Mjg0NDYsImlhdCI6MTczMzc1NTY0NiwiZW1haWwiOiJoa29kdXJpQHdoaXNzbGUuYWkifQ.E8xgZy_Rmpub0Q6P7YvvCJDtIC0_LxCApA0H-oIWLZQ -O v2_Malayalam_train.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf /external1/hkoduri/data/malayalam_processed/v2_Malayalam_valid.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf /external1/hkoduri/data/malayalam_processed/v2_Malayalam_train.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the data and path to audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"/home/compute/hkoduri/AI4Bharat/data/telugu_data/telugu_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"/external1/hkoduri/data/malayalam_processed/wavs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"/external1/hkoduri/data/malayalam_processed/wavs_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/external1/hkoduri/data/malayalam_processed/Malayalam/rv3/train'\n",
    "import glob\n",
    "json_list = glob.glob(path + '/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydub import AudioSegment\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "manifest_path = '/external1/hkoduri/data/malayalam_processed/manifest_train.jsonl'\n",
    "out_f = open(manifest_path, 'a', encoding='utf-8')\n",
    "for json_file in tqdm(json_list):\n",
    "  path = Path(json_file)\n",
    "  path_without_ext = path.with_suffix('')\n",
    "  f = open(json_file, 'r')\n",
    "  wavfile = AudioSegment.from_file(str(path_without_ext) + '.wav')\n",
    "  data = json.load(f)\n",
    "  for idx, chunk in enumerate(data['verbatim']):\n",
    "    chunk_segment = wavfile[chunk['start']*1000:chunk['end']*1000]\n",
    "    chunk_path = '/external1/hkoduri/data/malayalam_processed/wavs_train/'+ path.stem + '_' + str(idx) + '.wav'\n",
    "    chunk_segment.export(chunk_path, format=\"wav\")\n",
    "    manifest = {}\n",
    "    manifest['path'] = chunk_path\n",
    "    manifest['duration'] = chunk['end'] - chunk['start']\n",
    "    manifest['dialect'] = data['state']\n",
    "    manifest['gender'] = data['gender']\n",
    "    manifest['age_group'] = data['age_group']\n",
    "    manifest['intent'] = data['task_name']\n",
    "    manifest['text'] = chunk['text']\n",
    "    json.dump(manifest, out_f, ensure_ascii=False)\n",
    "    out_f.write(\"\\n\")\n",
    "  f.close()\n",
    "out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Path to the JSONL file\n",
    "jsonl_file = '/external1/hkoduri/data/malayalam_processed/manifest_train.jsonl'  # Update this path as needed\n",
    "\n",
    "# Path to save the intermediate JSON file\n",
    "intermediate_json_path = '/external1/hkoduri/data/malayalam_processed/intermediate_data.json'  # Update this path as needed\n",
    "\n",
    "# Collect all records from the JSONL file\n",
    "records = []\n",
    "\n",
    "# Read the JSONL file\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if not line.strip():  # Skip empty lines\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            continue\n",
    "        # Check if all required keys exist\n",
    "        required_keys = ['path', 'text', 'intent', 'age_group', 'gender', 'dialect']\n",
    "        if all(key in data for key in required_keys):\n",
    "            # Clean the text by removing annotations like [inhaling], [uhh], etc.\n",
    "            cleaned_text = re.sub(r'\\[.*?\\]', '', data['text']).strip()\n",
    "            # Replace multiple spaces with a single space\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "            records.append({\n",
    "                'path': data['path'],\n",
    "                'text': cleaned_text,\n",
    "                'intent': data['intent'],\n",
    "                'age_group': data['age_group'],\n",
    "                'gender': data['gender'],\n",
    "                'dialect': data['dialect']\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Missing required keys in data: {data}\")\n",
    "\n",
    "# Save the records to the intermediate JSON file\n",
    "os.makedirs(os.path.dirname(intermediate_json_path), exist_ok=True)\n",
    "with open(intermediate_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data extraction complete. Intermediate data saved to '{intermediate_json_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "import getpass\n",
    "import csv\n",
    "\n",
    "openai.api_key =''\n",
    "\n",
    "# Path to the intermediate JSON file generated by Script 1\n",
    "intermediate_json_path = '/external1/hkoduri/data/malayalam_processed/intermediate_data.json'  # Update this path as needed\n",
    "\n",
    "# Paths to save the output files\n",
    "output_json_path = '/external1/hkoduri/data/malayalam_processed/annotated_data.json'  # Update this path as needed\n",
    "output_csv_path = '/external1/hkoduri/data/malayalam_processed/annotated_data.csv'  # Update this path as needed\n",
    "\n",
    "# Load the records from the intermediate JSON file\n",
    "with open(intermediate_json_path, 'r', encoding='utf-8') as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "# Initialize annotated_records\n",
    "annotated_records = []\n",
    "\n",
    "# Check if output JSON file already exists and load it to resume from last point\n",
    "if os.path.exists(output_json_path):\n",
    "    with open(output_json_path, 'r', encoding='utf-8') as f_json:\n",
    "        annotated_records = json.load(f_json)\n",
    "    print(f\"Loaded {len(annotated_records)} existing annotated records.\")\n",
    "else:\n",
    "    print(\"No existing annotated records found. Starting fresh.\")\n",
    "\n",
    "# Create a set of paths for already annotated records to avoid duplicates\n",
    "annotated_paths = set(record['path'] for record in annotated_records)\n",
    "\n",
    "# Total number of records\n",
    "total_records = len(records)\n",
    "print(f\"Total sentences to process: {total_records}\")\n",
    "\n",
    "# Process records in batches\n",
    "batch_size = 10  # Set batch size to 10 as per your request\n",
    "total_batches = (total_records + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"Processing in batches of {batch_size} sentences.\")\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min((batch_num + 1) * batch_size, total_records)\n",
    "    batch_records = records[start_idx:end_idx]\n",
    "    print(f\"\\nProcessing batch {batch_num + 1}/{total_batches} (sentences {start_idx + 1} to {end_idx})...\")\n",
    "\n",
    "    # Prepare the batch of sentences to annotate\n",
    "    sentences_to_annotate = []\n",
    "    paths = []\n",
    "    intents = []\n",
    "    age_groups = []\n",
    "    genders = []\n",
    "    dialects = []\n",
    "\n",
    "    for idx, record in enumerate(batch_records, start=start_idx + 1):\n",
    "        path = record['path']\n",
    "\n",
    "        # Skip records that have already been annotated\n",
    "        if path in annotated_paths:\n",
    "            print(f\"Skipping already annotated sentence {idx}/{total_records}: {path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Preparing sentence {idx}/{total_records}: {path}\")\n",
    "        sentence = record['text']\n",
    "        intent = record['intent']\n",
    "        age_group = record['age_group']\n",
    "        gender = record['gender']\n",
    "        dialect = record['dialect']\n",
    "\n",
    "        sentences_to_annotate.append(sentence)\n",
    "        paths.append(path)\n",
    "        intents.append(intent)\n",
    "        age_groups.append(age_group)\n",
    "        genders.append(gender)\n",
    "        dialects.append(dialect)\n",
    "\n",
    "    if not sentences_to_annotate:\n",
    "        continue  # Skip if all sentences in this batch are already annotated\n",
    "\n",
    "    def annotate_sentences(sentences):\n",
    "        # Prepare the prompt with multiple sentences\n",
    "        prompt = f'''\n",
    "Given a list of sentences in Malayalam, annotate each sentence individually with the appropriate entity tags from the provided list. The sentences may relate to various actions such as managing tasks, controlling devices, sending notifications, scheduling events, updating information, or offering assistance.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "    Annotate each sentence separately.\n",
    "    Use the entity tags to indicate the start and end of each entity phrase in the sentence.\n",
    "    The tagging format is ENTITY_<type> to start an entity and there must be an END to close it.\n",
    "    Any spaces in the ENTITY_<type> must be replaced with _.\n",
    "    Only use the entity types provided in the list.\n",
    "    Do not add any additional text or explanations.\n",
    "    Ensure the output is a JSON array containing only the annotated sentences in the same order as the input sentences, without any markdown or formatting.\n",
    "\n",
    "Entities:\n",
    "\n",
    "[ \"PERSON NAME\", \"ORGANIZATION\", \"LOCATION\", \"DATE\", \"TIME\", \"DURATION\", \"EMAIL\", \"PHONE NUMBER\", \"ADDRESS\", \"CITY\", \"STATE\", \"COUNTRY\", \"ZIP CODE\", \"CURRENCY\", \"PRICE\", \"PRODUCT\", \"SERVICE\", \"BRAND\", \"EVENT\", \"PERCENTAGE\", \"AGE\", \"TEMPERATURE\", \"MEASUREMENT\", \"DISTANCE\", \"WEIGHT\", \"HEIGHT\", \"VOLUME\", \"SPEED\", \"LANGUAGE\", \"NATIONALITY\", \"RELIGION\", \"JOB TITLE\", \"COMPANY NAME\", \"DEVICE NAME\", \"OPERATING SYSTEM\", \"SOFTWARE VERSION\", \"COLOR\", \"SHAPE\", \"MATERIAL\", \"MODEL NUMBER\", \"LICENSE PLATE\", \"VEHICLE MAKE\", \"VEHICLE MODEL\", \"VEHICLE TYPE\", \"FLIGHT NUMBER\", \"HOTEL NAME\", \"BOOKING REFERENCE\", \"PAYMENT METHOD\", \"CREDIT CARD NUMBER\", \"ACCOUNT NUMBER\", \"INSURANCE PROVIDER\", \"POLICY NUMBER\", \"BANK NAME\", \"TAX ID\", \"SOCIAL SECURITY NUMBER\", \"DRIVER'S LICENSE\", \"PASSPORT NUMBER\", \"WEBSITE\", \"URL\", \"IP ADDRESS\", \"MAC ADDRESS\", \"USERNAME\", \"PASSWORD\", \"FOOD ITEM\", \"DRINK ITEM\", \"CUISINE\", \"INGREDIENT\", \"DISH NAME\", \"MENU ITEM\", \"ORDER NUMBER\", \"PAYMENT AMOUNT\", \"DELIVERY TIME\", \"DELIVERY DATE\", \"APPOINTMENT DATE\", \"APPOINTMENT TIME\", \"ROOM NUMBER\", \"HOSPITAL NAME\", \"DOCTOR NAME\", \"SYMPTOM\", \"DIAGNOSIS\", \"MEDICATION\", \"DOSAGE\", \"ALLERGY\", \"PRESCRIPTION\", \"TEST NAME\", \"TEST RESULT\", \"INSURANCE PLAN\", \"CLAIM NUMBER\", \"POLICY HOLDER\", \"BENEFICIARY\", \"RELATIONSHIP\", \"EMERGENCY CONTACT\", \"PROJECT NAME\", \"TASK\", \"MEETING\", \"AGENDA\", \"ACTION ITEM\", \"DEADLINE\", \"PRIORITY\", \"FEEDBACK\", \"REVIEW\", \"RATING\", \"COMPLAINT\", \"QUESTION\", \"RESPONSE\" ]\n",
    "\n",
    "Example:\n",
    "\n",
    "Input Sentences:\n",
    "[\n",
    "    \"ഈ കാബ്ബേജ് മത്തങ്ങ അങ്ങനെയൊക്കെ ഇള്ള സാധങ്ങള് എനിക്ക് കിട്ടുന്നുണ്ട് ഇതും ഇതിന് എൻ്റെ അടുത്തുള്ള ഒരു ചന്തയ്ണ്ട് മാർക്കെറ്റില് പോയി ഇത് വിൽക്കുകയും അത്യാവശ്യം നല്ല പൈസ കിട്ടുകയും ചെയ്യുന്നു ഇതിന് വന്നിട്ട് ഞാൻ യൂസ് ചെയ്യുന്നത് ജൈവ\",\n",
    "    \"ഈ ഓടാൻ പോകുന്ന ആളുടെ ശരീരത്തിൽ പറഞ്ഞാൽ കൊഴുപ്പൊന്നും ഉണ്ടാകില്ല അവരെപ്പോഴും വർക്കൗട്ട് ചെയ്യുന്നത് കൊണ്ട് തന്നെ എപ്പളും ഫിറ്റായിരിക്കും\",\n",
    "    \"ഓക്കേ അപ്പൊ ബില്ല് കിട്ടീട്ടുണ്ട് ആഹ് ഓക്കേ മാം പ്രൊഡക്ടിന് എന്തൊക്കെ ആയിരുന്നു ഡാമേജ് കാര്യങ്ങൾ ഇണ്ടായിരുന്നത്\",\n",
    "    \"കമ്പ് നട്ട് കമ്പ് നട്ട് അങ്ങനെ രണ്ടുമൂന്ന് ചെടിയൊക്കെ ആക്കി എടുക്കും പിന്നെ എലച്ചെടികളൊക്കെ കൂടുതലായിട്ട്ണ്ടെങ്കില് അതിൻ്റെയൊക്കെ തണ്ട് ചെറുതായിട്ട് മുറിച്ച് അല്ലെങ്കില് വിത്തൊക്കെ ഇണ്ടെങ്കില് അത് തന്നെയെടുത്ത് സൂക്ഷിച്ച് വെച്ച് പാകി അങ്ങനെയാണ് ചെടി കൂടുതലായും ഞാന് പുതിയ പുതിയ ചെടികള് വളർത്തിയെടുക്കുന്നത്\",\n",
    "    \"അറ്റത്തു നിന്ന് ഒരു എന്താ പറയുക പ്ലാസ്റ്റിക് ഉരുകുന്നപോലേന്ന് പറയാൻ പറ്റത്തില്ല ഒരു പ്ലാസ്റ്റിക്കിൻ്റെ ആ ഒരു ടെക്സ്ചർ അങ്ങ് മാറി\"\n",
    "]\n",
    "\n",
    "Tagged Output:\n",
    "[\n",
    "    \"ഈ ENTITY_FOOD_ITEM കാബ്ബേജ് END ENTITY_FOOD_ITEM മത്തങ്ങ END അങ്ങനെയൊക്കെ ഇള്ള സാധങ്ങള് എനിക്ക് കിട്ടുന്നുണ്ട് ഇതും ഇതിന് എൻ്റെ അടുത്തുള്ള ഒരു ENTITY_LOCATION ചന്തയ്ണ്ട് മാർക്കെറ്റ് END ൽ പോയി ഇത് വിൽക്കുകയും അത്യാവശ്യം ENTITY_PRICE നല്ല പൈസ END കിട്ടുകയും ചെയ്യുന്നു ഇതിന് വന്നിട്ട് ഞാൻ യൂസ് ചെയ്യുന്നത് ജൈവ\",\n",
    "    \"ഈ ഓടാൻ പോകുന്ന ആളുടെ ശരീരത്തിൽ പറഞ്ഞാൽ കൊഴുപ്പൊന്നും ഉണ്ടാകില്ല അവരെപ്പോഴും വർക്കൗട്ട് ചെയ്യുന്നത് കൊണ്ട് തന്നെ എപ്പളും ഫിറ്റായിരിക്കും\",\n",
    "    \"ഓക്കേ അപ്പൊ ബില്ല് കിട്ടീട്ടുണ്ട് ആഹ് ഓക്കേ മാം ENTITY_PRODUCT പ്രൊഡക്ടിന് END എന്തൊക്കെ ആയിരുന്നു ഡാമേജ് കാര്യങ്ങൾ ഇണ്ടായിരുന്നത്\",\n",
    "    \"കമ്പ് നട്ട് കമ്പ് നട്ട് അങ്ങനെ രണ്ടുമൂന്ന് ചെടിയൊക്കെ ആക്കി എടുക്കും പിന്നെ ENTITY_INGREDIENT എലച്ചെടികളൊക്കെ END കൂടുതലായിട്ട്ണ്ടെങ്കില് അതിൻ്റെയൊക്കെ തണ്ട് ചെറുതായിട്ട് മുറിച്ച് അല്ലങ്കിൽ വിത്തൊക്കെ ഇണ്ടെങ്കില് അത് തന്നെയടുത്ത് സൂക്ഷിച്ച് വെച്ച് പാകി അങ്ങനെയാണ് ചെടി ശ്രദ്ധിച്ച് പുതിയ പുതിയ ചെടികള് വളർത്തിയെടുക്കുന്നത്\",\n",
    "    \"അറ്റത്തു നിന്ന് ഒരു എന്താ പറയുക ENTITY_MATERIAL പ്ലാസ്റ്റിക് END ഉരുകുന്നപോലേന്ന് പറയാൻ പറ്റത്തില്ല ഒരു ENTITY_MATERIAL പ്ലാസ്റ്റിക് END ന്റെ ആ ഒരു ടെക്സ്ചർ അങ്ങ് മാറി\"\n",
    "\n",
    "]\n",
    "\n",
    "Sentences to Annotate:\n",
    "{json.dumps(sentences, ensure_ascii=False)}\n",
    "'''\n",
    "        try:\n",
    "            #client = openai.OpenAI(api_key='')\n",
    "            response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4o-mini-2024-07-18\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=4096,\n",
    "                    n=1,\n",
    "                    stop=None,\n",
    "                    temperature=0.5,\n",
    "                )\n",
    "                # Get the assistant's reply\n",
    "            assistant_reply = dict(response).get('choices')[0].message.content\n",
    "            try:\n",
    "                annotated_sentences = json.loads(assistant_reply)\n",
    "                if isinstance(annotated_sentences, list):\n",
    "                    return annotated_sentences\n",
    "                else:\n",
    "                    print(\"Assistant did not return a list. Fallback to raw reply.\")\n",
    "                    return [assistant_reply] * len(sentences)  # Fallback\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"JSON decoding failed. Fallback to raw replies.\")\n",
    "                return [assistant_reply] * len(sentences)  # Fallback\n",
    "        except Exception as e:\n",
    "            print(f\"Error annotating batch starting at sentence {start_idx + 1}: {e}\")\n",
    "            return sentences  # Return original sentences if annotation fails\n",
    "\n",
    "    annotated_batch_sentences = annotate_sentences(sentences_to_annotate)\n",
    "\n",
    "    # Ensure we have annotations for all sentences\n",
    "    if len(annotated_batch_sentences) != len(sentences_to_annotate):\n",
    "        print(\"Mismatch in number of annotated sentences. Using original sentences.\")\n",
    "        annotated_batch_sentences = sentences_to_annotate\n",
    "\n",
    "    # Save annotations\n",
    "    for i in range(len(annotated_batch_sentences)):\n",
    "        annotated_sentence = annotated_batch_sentences[i]\n",
    "        path = paths[i]\n",
    "        intent = intents[i]\n",
    "        age_group = age_groups[i]\n",
    "        gender = genders[i]\n",
    "        dialect = dialects[i]\n",
    "\n",
    "        # Construct the final output\n",
    "        def format_tag(tag_type, value):\n",
    "            return f\"{tag_type}_{value.upper().replace(' ', '_').replace('-', '_').replace('/', '_')}\"\n",
    "\n",
    "        final_output = f\"{annotated_sentence} {format_tag('INTENT', intent)} {format_tag('AGE', age_group)} {format_tag('GENDER', gender)} {format_tag('DIALECT', dialect)}\"\n",
    "\n",
    "        annotated_records.append({'path': path, 'Final Output': final_output})\n",
    "\n",
    "        # Update the set of annotated paths\n",
    "        annotated_paths.add(path)\n",
    "\n",
    "    # Save progress after each batch\n",
    "    # Write to JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f_json:\n",
    "        json.dump(annotated_records, f_json, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Write to CSV file\n",
    "    csv_columns = ['path', 'Final Output']\n",
    "    with open(output_csv_path, 'w', encoding='utf-8', newline='') as f_csv:\n",
    "        writer = csv.DictWriter(f_csv, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(annotated_records)\n",
    "\n",
    "    # Optional: Sleep to respect rate limits\n",
    "    # time.sleep(1)  # Adjust or remove as needed\n",
    "\n",
    "print(\"\\nAnnotation complete.\")\n",
    "print(f\"Files saved as '{os.path.basename(output_json_path)}' and '{os.path.basename(output_csv_path)}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
