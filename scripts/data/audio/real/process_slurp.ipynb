{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process SLURP dataset for 1SSI: OneStep speech instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from pathlib import PurePath\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "from nemo.collections import nlp as nemo_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-15 02:17:05 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/punctuation_en_distilbert/versions/1.0.0rc1/files/punctuation_en_distilbert.nemo to /root/.cache/torch/NeMo/NeMo_1.21.0rc0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo\n",
      "[NeMo I 2023-11-15 02:17:21 common:913] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2023-11-15 02:17:23 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: distilbert-base-uncased, vocab_file: /tmp/tmplhzxjccm/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7902b75cbe4c46c8aab6167101f0f059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)cased/resolve/main/tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f976aa0803c64455b42d597a24ca55a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)rt-base-uncased/resolve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badf1d67e95a433dbbdb61740de4be27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)bert-base-uncased/resolve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "[NeMo W 2023-11-15 02:17:24 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2023-11-15 02:17:24 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-15 02:17:24 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-15 02:17:24 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-15 02:17:24 nlp_overrides:446] megatron-core was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/NeMo#megatron-gpt.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cde37ac5230429f9e59d7ee73ee1f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-15 02:17:27 punctuation_capitalization_model:719] The artifact `class_labels.punct_labels_file` was not found in checkpoint. Will rely on `punct_label_ids` parameter\n",
      "[NeMo W 2023-11-15 02:17:27 punctuation_capitalization_model:741] The artifact `class_labels.capit_labels_file` was not found in checkpoint. Will rely on `capit_label_ids` parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-15 02:17:27 save_restore_connector:249] Model PunctuationCapitalizationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.21.0rc0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo.\n"
     ]
    }
   ],
   "source": [
    "### Intitiate text normalizer and puctuator\n",
    "normalizer = Normalizer(input_case='lower_cased', lang=\"en\")\n",
    "punctuator = nemo_nlp.models.PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_distilbert\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    normalized = normalizer.normalize(text, verbose=True, punct_post_process=True)\n",
    "    normalized = [normalized]\n",
    "    norm_punctuated = punctuator.add_punctuation_capitalization(normalized)[0]\n",
    "    return norm_punctuated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurp_annotations = Path(\"/n/disk1/audio_datasets/slurp/dataset/slurp/\")\n",
    "train_annotations = slurp_annotations / Path(\"train.jsonl\")\n",
    "dev_annotations = slurp_annotations / Path(\"devel.jsonl\")\n",
    "test_annotations = slurp_annotations / Path(\"test.jsonl\")\n",
    "\n",
    "audio_real = Path(\"/n/disk1/audio_datasets/slurp/audio/slurp_real\")\n",
    "audio_synth = Path(\"/n/disk1/audio_datasets/slurp/audio/slurp_synth/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_entity_format(text, tagdict):\n",
    "    # Regular expression to find any entity type pattern\n",
    "    pattern = r'\\[([a-zA-Z_]+) : ([^\\]]+)\\]'\n",
    "\n",
    "    # Function to replace the found pattern\n",
    "    def replace_pattern(match):\n",
    "        entity_type = match.group(1).strip().upper()  # Convert entity type to uppercase\n",
    "        entity_value = match.group(2).strip()\n",
    "\n",
    "        begin_tag = \"B-{entity_type}\"\n",
    "        end_tag =  \"E-{entity_type}\"\n",
    "\n",
    "        if begin_tag not in tagdict.keys():\n",
    "            \n",
    "            tagdict[begin_tag] = \"DUMMY-\"+str(len(tagdict.keys()) + 1)\n",
    "            begin_tag = tagdict[begin_tag]\n",
    "        else:\n",
    "            begin_tag = tagdict[begin_tag]\n",
    "\n",
    "        if end_tag not in tagdict.keys():\n",
    "            \n",
    "            tagdict[end_tag] = \"DUMMY-\"+str(len(tagdict.keys()) + 1)\n",
    "            end_tag = tagdict[end_tag]\n",
    "        else:\n",
    "            end_tag = tagdict[end_tag]\n",
    "\n",
    "        return f\"{begin_tag} {entity_value} {end_tag}\"\n",
    "\n",
    "    # Replace all occurrences of the pattern in the text\n",
    "    converted_text = re.sub(pattern, replace_pattern, text)\n",
    "\n",
    "    return converted_text, tagdict\n",
    "\n",
    "\n",
    "\n",
    "def add_entity_tags(input1, input2):\n",
    "    # Find all entities in input2\n",
    "    entities = re.findall(r'B-([A-Z_]+) (.*?) E-\\1', input2)\n",
    "\n",
    "    # Function to handle punctuation around the entity\n",
    "    def replace_entity(match):\n",
    "        before, entity, after = match.groups()\n",
    "        return f\"{before}B-{entity_type} {entity_value} E-{entity_type}{after}\"\n",
    "\n",
    "    # Replace the text in input1 with tagged text from input2\n",
    "    for entity in entities:\n",
    "        entity_type, entity_value = entity\n",
    "        # Pattern to include possible punctuation around the entity\n",
    "        pattern = r'(\\W?)(\\b' + re.escape(entity_value) + r'\\b)(\\W?)'\n",
    "        input1 = re.sub(pattern, replace_entity, input1, 1, re.IGNORECASE)\n",
    "\n",
    "    return input1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1bb32573df4d069b0112fd9f37e24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)/Hubert_emotion/resolve/main/config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb56776caee843e7898519dd193c4c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rajaram1996/Hubert_emotion were not used when initializing HubertForSpeechClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSpeechClassification were not initialized from the model checkpoint at Rajaram1996/Hubert_emotion and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d53a2c517f4f959e799bed8929db70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)60/resolve/main/preprocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor\n",
    "from AudioEmotionClassification.models import Wav2Vec2ForSpeechClassification, HubertForSpeechClassification\n",
    "\n",
    "emotion_model = HubertForSpeechClassification.from_pretrained(\"Rajaram1996/Hubert_emotion\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "sampling_rate=16000 # defined by the model; must convert mp3 to this rate.\n",
    "config = AutoConfig.from_pretrained(\"Rajaram1996/Hubert_emotion\")\n",
    "\n",
    "def speech_file_to_array_fn(path, sampling_rate):\n",
    "    speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(_sampling_rate, sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "\n",
    "def predict(path, sampling_rate):\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in\n",
    "               enumerate(scores)]\n",
    "    return outputs\n",
    "\n",
    "def get_emotion_labels(audio_file, sampling_rate=16000, score=50.0):\n",
    "    sound_array = speech_file_to_array_fn(audio_file, sampling_rate)\n",
    "    \n",
    "    inputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(\"cpu\").float() for key in inputs}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "    outputs = [{\n",
    "        \"emo\": config.id2label[i],\n",
    "        \"score\": round(score * 100, 1)}\n",
    "        for i, score in enumerate(scores)\n",
    "    ]\n",
    "\n",
    "    #[{'emo': 'female_neutral', 'score': 73.9}, {'emo': 'female_happy', 'score': 24.8}]\n",
    "    emotion_labels = [row for row in sorted(outputs, key=lambda x:x[\"score\"], reverse=True) if row['score'] != '0.0%'][:2]\n",
    "\n",
    "    all_labels = []\n",
    "    for emotion_dict in emotion_labels:\n",
    "        label = emotion_dict['emo'].split(\"_\")[1].upper()\n",
    "        score = emotion_dict['score']\n",
    "\n",
    "        if score > 50.0:\n",
    "            all_labels.append(label)\n",
    "\n",
    "    return all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_ENTITIES = {}\n",
    "\n",
    "\n",
    "def jsonl_process(jsonlfile,audiofolder, tagdictfile):\n",
    "\n",
    "    print(jsonlfile)\n",
    "\n",
    "    wavfolder = str(audiofolder) + \"-wav\"\n",
    "    os.system(\"mkdir -p \"+wavfolder)\n",
    "    wavfolder = Path(wavfolder)\n",
    "\n",
    "    jsonlfileread = open(str(jsonlfile),'r').readlines()\n",
    "\n",
    "    \n",
    "    manifest = open(jsonlfile.name.replace(jsonlfile.suffix, \"\") + \".json\",'w')\n",
    "\n",
    "    tagdict = json.load(open(tagdictfile,'r'))\n",
    "\n",
    "    for line in jsonlfileread:\n",
    "\n",
    "        line = json.loads(line)\n",
    "        print(line)\n",
    "        annotation = line['sentence_annotation']\n",
    "        text = line['sentence']\n",
    "        text_clean = normalize(text)\n",
    "        text_tagged, tagdict = convert_entity_format(line['sentence_annotation'], tagdict)\n",
    "        text_clean_tagged = add_entity_tags(text_clean, text_tagged)\n",
    "        \n",
    "        intent = line['intent'].upper()\n",
    "\n",
    "        recordings = line['recordings']\n",
    "\n",
    "        print(\"Final text:\", text_clean_tagged)\n",
    "\n",
    "        for recording in recordings:\n",
    "            audiofile = recording['file']\n",
    "            audiofilepath = audiofolder / Path(audiofile)\n",
    "\n",
    "            audiofile = PurePath(audiofile)\n",
    "            filekey = audiofile.name.replace(audiofile.suffix, \"\")\n",
    "            wavfilepath = str(wavfolder) + \"/\" + filekey + \".wav\"\n",
    "            #flac_tmp_audio_data = AudioSegment.from_file(audiofilepath, audiofilepath.suffix[1:])\n",
    "            #flac_tmp_audio_data.export(wavfilepath, format=\"wav\")\n",
    "            \n",
    "            \n",
    "            print(audiofilepath)\n",
    "\n",
    "            sample_dict = {}\n",
    "            sample_dict['audiofilepath'] = wavfilepath\n",
    "            sample_dict['text'] = text_clean\n",
    "            sample_dict['tagged_text'] = text_clean\n",
    "\n",
    "            flac_tmp_audio_data = AudioSegment.from_file(audiofilepath, audiofilepath.suffix[1:])\n",
    "            flac_tmp_audio_data.export(wavfilepath, format=\"wav\")\n",
    "            sample_dict['instruction'] = \"transcribe speech\"\n",
    "\n",
    "            json.dump(sample_dict, manifest)\n",
    "            manifest.write(\"\\n\")\n",
    "\n",
    "            sample_dict['tagged_text'] = text_clean_tagged\n",
    "            sample_dict['instruction'] = \"transcribe and mark entities\"\n",
    "            json.dump(sample_dict, manifest)\n",
    "            manifest.write(\"\\n\")\n",
    "\n",
    "\n",
    "            emotion_labels = get_emotion_labels(audio_file=wavfilepath, sampling_rate=16000)\n",
    "            emotion_labels = ' '.join(emotion_labels)\n",
    "\n",
    "            final_transcription = text_clean_tagged + \" \" + emotion_labels\n",
    "\n",
    "            sample_dict['tagged_text'] = final_transcription\n",
    "            sample_dict['instruction'] = \"transcribe, mark entitites and track speaker emotion\"\n",
    "            json.dump(sample_dict, manifest)\n",
    "            manifest.write(\"\\n\")\n",
    "\n",
    "            sample_dict['tagged_text'] = text_clean_tagged + \" \" + intent\n",
    "            sample_dict['instruction'] = \"transcribe, mark entitites, get speaker intent\"\n",
    "            json.dump(sample_dict, manifest)\n",
    "            manifest.write(\"\\n\")\n",
    "\n",
    "            sample_dict['tagged_text'] = final_transcription + \" \" + intent\n",
    "            sample_dict['instruction'] = \"transcribe, mark entitites, get intent and emotion labels\"\n",
    "            json.dump(sample_dict, manifest)\n",
    "            manifest.write(\"\\n\")        \n",
    "    \n",
    "    manifest.close()         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_process(jsonlfile=train_annotations, audiofolder=audio_real)\n",
    "#jsonl_process(jsonlfile=train_annotations, audiofolder=audio_real)\n",
    "#jsonl_process(jsonlfile=train_annotations, audiofolder=audio_real)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
