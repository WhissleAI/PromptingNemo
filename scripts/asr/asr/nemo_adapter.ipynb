{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from a local .nemo file\n",
    "\n",
    "MODEL_ROOT = \"/external2/models/hf/stt_en_conformer_ctc_large/\"\n",
    "MODEL_ROOT = Path(MODEL_ROOT)\n",
    "model_path = MODEL_ROOT / \"stt_en_conformer_ctc_large.nemo\"\n",
    "\n",
    "cfg = nemo_asr.models.ASRModel.restore_from(restore_path=model_path, return_config=True)\n",
    "\n",
    "# If you need to get the configuration, you can access it from the loaded model\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nemo.core import adapter_mixins\n",
    "\n",
    "# # Utility method to check and update the model config\n",
    "# def update_model_config_to_support_adapter(model_cfg):\n",
    "#     with open_dict(model_cfg):\n",
    "#         adapter_metadata = adapter_mixins.get_registered_adapter(model_cfg.encoder._target_)\n",
    "#         if adapter_metadata is not None:\n",
    "#             model_cfg.encoder._target_ = adapter_metadata.adapter_class_path\n",
    "\n",
    "#     print(\"Updated encoder _target_ model :\", model_cfg.encoder._target_)\n",
    "#     return model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg = update_model_config_to_support_adapter(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nemo_asr.models.ASRModel.restore_from(model_path, override_config_path=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/ksingla/workspace/PromptingNemo/data_v2/synthetic/processed/\"\n",
    "TRAIN_MANIFEST = os.path.join(data_dir, \"train.json\")\n",
    "TEST_MANIFEST = os.path.join(data_dir, \"valid.json\")\n",
    "ALL_TAGS = os.path.join(data_dir, \"alltags_uniq.txt\")\n",
    "taglist = []\n",
    "with open(ALL_TAGS, 'r') as f:\n",
    "    for line in f:\n",
    "        word, tag = line.split()\n",
    "        taglist.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import logging\n",
    "\n",
    "def train_sentencepiece_tokenizer(manifest_file, tokenizer_folder, special_tokens=None, vocab_size=5000):\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(\"Starting the tokenizer training process\")\n",
    "\n",
    "    # Step 1: Read the manifest file and extract text data\n",
    "    def read_manifest(manifest_path):\n",
    "        with open(manifest_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        return [json.loads(line)['text'] for line in lines]\n",
    "    \n",
    "    logging.info(\"Reading manifest file\")\n",
    "    text_data = read_manifest(manifest_file)\n",
    "    logging.info(f\"Extracted {len(text_data)} sentences from the manifest file\")\n",
    "    \n",
    "    # Step 2: Save the extracted text to a temporary file\n",
    "    if not os.path.exists(tokenizer_folder):\n",
    "        os.makedirs(tokenizer_folder)\n",
    "    \n",
    "    temp_text_file = os.path.join(tokenizer_folder, 'text_data.txt')\n",
    "    logging.info(f\"Saving extracted text to {temp_text_file}\")\n",
    "    with open(temp_text_file, 'w') as f:\n",
    "        for sentence in text_data:\n",
    "            f.write(sentence + '\\n')\n",
    "    \n",
    "    # Step 3: Train the SentencePiece tokenizer with special tokens if provided\n",
    "    model_prefix = os.path.join(tokenizer_folder, 'tokenizer')\n",
    "    \n",
    "    # Prepare special tokens string\n",
    "    if special_tokens:\n",
    "        user_defined_symbols = ','.join(special_tokens)\n",
    "        logging.info(f\"Special tokens provided: {special_tokens}\")\n",
    "        logging.info(\"Starting SentencePiece training with special tokens\")\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=temp_text_file, \n",
    "            model_prefix=model_prefix, \n",
    "            vocab_size=vocab_size,\n",
    "            user_defined_symbols=user_defined_symbols\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\"Starting SentencePiece training without special tokens\")\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=temp_text_file, \n",
    "            model_prefix=model_prefix, \n",
    "            vocab_size=vocab_size\n",
    "        )\n",
    "    \n",
    "    # Step 4: Return the paths to the tokenizer model and vocab files\n",
    "    model_file = f\"{model_prefix}.model\"\n",
    "    vocab_file = f\"{model_prefix}.vocab\"\n",
    "\n",
    "    logging.info(f\"Tokenizer training completed\")\n",
    "    logging.info(f\"Model file: {model_file}\")\n",
    "    logging.info(f\"Vocab file: {vocab_file}\")\n",
    "\n",
    "    # Step 5: Create a vocab.txt file\n",
    "    vocab_txt_file = os.path.join(tokenizer_folder, 'vocab.txt')\n",
    "    logging.info(f\"Creating vocab.txt file at {vocab_txt_file}\")\n",
    "    with open(vocab_file, 'r') as vf, open(vocab_txt_file, 'w') as vtf:\n",
    "        for line in vf:\n",
    "            token = line.split('\\t')[0]\n",
    "            vtf.write(token + '\\n')\n",
    "    \n",
    "    logging.info(f\"vocab.txt file created at {vocab_txt_file}\")\n",
    "    \n",
    "    return model_file, vocab_file, vocab_txt_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglist = open(ALL_TAGS, 'r').read().splitlines()\n",
    "taglist = [tag.split()[1] for tag in taglist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentencepiece_tokenizer(TRAIN_MANIFEST, \"/home/ksingla/workspace/PromptingNemo/data_v2/synthetic/processed/tokenizer\", special_tokens=taglist, vocab_size=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def generate_sentencepiece_model_pb2(script_dir, proto_file_path):\n",
    "    # Construct the command\n",
    "    command = [\n",
    "        'protoc',\n",
    "        f'--python_out={script_dir}',\n",
    "        proto_file_path\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Run the command\n",
    "        subprocess.run(command, check=True)\n",
    "        print(\"Successfully generated sentencepiece_model_pb2.py\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error generating sentencepiece_model_pb2.py: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def edit_spt_model(input_file, output_folder, tokens, vocab_file, vocab_txt_file, is_userdefined=False):\n",
    "    from sentencepiece_model_pb2 import ModelProto  # Ensure this import is after the proto generation\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    output_model_file = os.path.join(output_folder, 'tokenizer.model')\n",
    "    output_vocab_file = os.path.join(output_folder, 'tokenizer.vocab')\n",
    "    output_vocab_txt_file = os.path.join(output_folder, 'vocab.txt')\n",
    "\n",
    "    token_type = 3\n",
    "    if is_userdefined:\n",
    "        token_type = 4\n",
    "\n",
    "    model = ModelProto()\n",
    "    model.ParseFromString(open(input_file, 'rb').read())\n",
    "\n",
    "    existing_tokens = {piece.piece for piece in model.pieces}\n",
    "\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in existing_tokens:\n",
    "            logging.warning(f\"Special Token '{token}' already exists in the input model, skipping.\")\n",
    "            continue\n",
    "        piece = model.SentencePiece(piece=token, score=0.0, type=token_type)\n",
    "        model.pieces.append(piece)\n",
    "        new_tokens.append(token)\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    try:\n",
    "        sp.LoadFromSerializedProto(model.SerializeToString())\n",
    "        for token in new_tokens:\n",
    "            id = sp.piece_to_id(token)\n",
    "            logging.info(f\"Created token '{token}' at ID {id}\")\n",
    "        logging.info(f\"New tokenizer vocab size: {sp.get_piece_size()}\")\n",
    "    except:\n",
    "        logging.error(\"Could not appropriately configure new tokenizer. Verify if the special tokens already exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    with open(output_model_file, 'wb') as outf:\n",
    "        outf.write(model.SerializeToString())\n",
    "\n",
    "    logging.info(f\"Created new tokenizer at: {output_model_file}\")\n",
    "\n",
    "    # Read the original vocab file and append the new tokens\n",
    "    with open(vocab_file, 'r') as original_vocab_file:\n",
    "        original_vocab = original_vocab_file.readlines()\n",
    "\n",
    "    with open(output_vocab_file, 'w') as updated_vocab_file:\n",
    "        updated_vocab_file.writelines(original_vocab)\n",
    "        for token in new_tokens:\n",
    "            updated_vocab_file.write(f\"{token}\\n\")\n",
    "\n",
    "    # Update vocab.txt\n",
    "    with open(vocab_txt_file, 'r') as original_vocab_txt_file:\n",
    "        original_vocab_txt = original_vocab_txt_file.readlines()\n",
    "\n",
    "    with open(output_vocab_txt_file, 'w') as updated_vocab_txt_file:\n",
    "        updated_vocab_txt_file.writelines(original_vocab_txt)\n",
    "        for token in new_tokens:\n",
    "            updated_vocab_txt_file.write(f\"{token}\\n\")\n",
    "\n",
    "    logging.info(f\"Updated vocab files: {output_vocab_file}, {output_vocab_txt_file}\")\n",
    "\n",
    "def update_model_config(model, new_model_path):\n",
    "    model['cfg']['tokenizer']['model_path'] = new_model_path\n",
    "    logging.info(f\"Updated model configuration with new tokenizer model path: {new_model_path}\")\n",
    "\n",
    "\n",
    "# # Define input and output paths\n",
    "# input_folder = MODEL_ROOT / \"tokenizer\"\n",
    "# output_folder = MODEL_ROOT / \"new_tokenizer\"\n",
    "\n",
    "\n",
    "# #input_folder = '/external/ksingla/models/nemo/stt_en_conformer_ctc_small/tokenizer'\n",
    "# #output_folder = '/external/ksingla/models/nemo/stt_en_conformer_ctc_small/new_tokenizer'\n",
    "# #proto_dir = '/path/to/save/proto'  # Define the actual path where the proto file should be saved\n",
    "# #proto_file = '/path/to/sentencepiece_model.proto'  # Define the actual path to the sentencepiece_model.proto file\n",
    "\n",
    "# input_file = input_folder / 'tokenizer.model'\n",
    "# vocab_file = input_folder / 'tokenizer.vocab'\n",
    "# vocab_txt_file = input_folder / 'vocab.txt'\n",
    "\n",
    "# # input_file = os.path.join(input_folder, 'tokenizer.model')\n",
    "# # vocab_file = os.path.join(input_folder, 'tokenizer.vocab')\n",
    "# # vocab_txt_file = os.path.join(input_folder, 'vocab.txt')\n",
    "\n",
    "# # Include all single-digit integers in the tokens list\n",
    "# punctuations = ['.', ',', '?', '!', ';', ':', '-', '(', ')', '[', ']', '{', '}', '<', '>', '/', '\\\\', '|', '@', '#', '$', '%', '^', '&', '*', '+', '=', '~', '`', '_', '\"', \"'\"]\n",
    "# tokens = taglist + [str(i) for i in range(10)] + punctuations\n",
    "# is_userdefined = True\n",
    "\n",
    "# # Step 1: Generate the sentencepiece_model_pb2.py file\n",
    "# #generate_sentencepiece_model_pb2(proto_dir, proto_file)\n",
    "\n",
    "# # Step 2: Edit the SentencePiece model\n",
    "# edit_spt_model(input_file, output_folder, tokens, vocab_file, vocab_txt_file, is_userdefined)\n",
    "\n",
    "# Step 3: Load the model configuration and update it\n",
    "# model_config_file = '/path/to/model/config.json'  # Define the actual path to the model config file\n",
    "# with open(model_config_file, 'r') as f:\n",
    "#     model = json.load(f)\n",
    "\n",
    "# new_model_path = os.path.join(output_folder, 'tokenizer.model')\n",
    "# update_model_config(model, new_model_path)\n",
    "\n",
    "# # Save the updated model configuration\n",
    "# with open(model_config_file, 'w') as f:\n",
    "#     json.dump(model, f, indent=4)\n",
    "\n",
    "#logging.info(f\"Updated model configuration saved to: {model_config_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.change_vocabulary(\"/home/ksingla/workspace/PromptingNemo/data_v2/synthetic/processed/tokenizer/\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "max_steps = 600000\n",
    "\n",
    "trainer = Trainer(devices=1, accelerator=accelerator, max_steps=max_steps,\n",
    "                  enable_checkpointing=False, logger=False,\n",
    "                  log_every_n_steps=50, check_val_every_n_epoch=1, accumulate_grad_batches=8)\n",
    "\n",
    "model.set_trainer(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility method\n",
    "import json\n",
    "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_dict(model.cfg):\n",
    "  # Train Dataloader\n",
    "  model.cfg.train_ds.manifest_filepath = TRAIN_MANIFEST\n",
    "  model.cfg.train_ds.batch_size = 12\n",
    "  model.cfg.train_ds.is_tarred = False\n",
    "  model.cfg.train_ds.tarred_audio_filepaths = None\n",
    "  model.cfg.train_ds.num_workers = 8  # Adding num_workers for training dataloader\n",
    "\n",
    "  model.cfg.validation_ds.manifest_filepath = TEST_MANIFEST\n",
    "  model.cfg.validation_ds.batch_size = 12\n",
    "  model.cfg.validation_ds.num_workers = 8  # Adding num_workers for training dataloader\n",
    "\n",
    "model.setup_training_data(model.cfg.train_ds)\n",
    "model.setup_multiple_validation_data(model.cfg.validation_ds)\n",
    "model.setup_multiple_test_data(model.cfg.validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_dict(model.cfg):\n",
    "  # Spec Augment\n",
    "  model.cfg.spec_augment.freq_masks = model.cfg.spec_augment.freq_masks  # Can be changed\n",
    "  model.cfg.spec_augment.freq_width = model.cfg.spec_augment.freq_width  # Can be changed\n",
    "  model.cfg.spec_augment.time_masks = model.cfg.spec_augment.time_masks  # Can be changed\n",
    "  model.cfg.spec_augment.time_width = model.cfg.spec_augment.time_width  # Can be changed\n",
    "\n",
    "model.spec_augmentation = model.from_config_dict(model.cfg.spec_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'optim' in model.cfg:\n",
    "  print(OmegaConf.to_yaml(model.cfg.optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_dict(model.cfg):\n",
    "  model.cfg.optim.lr = 0.1\n",
    "  model.cfg.optim.weight_decay = 0.0001\n",
    "  model.cfg.optim.sched.warmup_steps = 1000\n",
    "\n",
    "model.setup_optimization(model.cfg.optim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if hasattr(model, 'adapter_module_names'):\n",
    "#   print(model.adapter_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for module in model.children():\n",
    "#   if hasattr(module, 'get_accepted_adapter_types'):\n",
    "#     types = module.get_accepted_adapter_types()\n",
    "#     print(\"Module : \", module.__class__.__name__)\n",
    "\n",
    "#     for tp in types:\n",
    "#       print(tp)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nemo.collections.common.parts.adapter_modules import LinearAdapterConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adapter_name = \"AN4\" #@param {type:\"string\"}\n",
    "# adapter_dim = 32 #@param {type:\"integer\"}\n",
    "# adapter_activation = \"swish\" #@param {type:\"string\"}\n",
    "# adapter_norm_position = \"pre\" #@param [\"pre\", \"post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapter_cfg = LinearAdapterConfig(\n",
    "#     in_features=model.cfg.encoder.d_model,  # conformer specific model dim. Every layer emits this dim at its output.\n",
    "#     dim=adapter_dim,  # the bottleneck dimension of the adapter\n",
    "#     activation=adapter_activation,  # activation used in bottleneck block\n",
    "#     norm_position=adapter_norm_position,  # whether to use LayerNorm at the beginning or the end of the adapter\n",
    "# )\n",
    "# print(adapter_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add_adapter(name=adapter_name, cfg=adapter_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_enabled_adapters(enabled=False)  # disable all adapters\n",
    "# model.set_enabled_adapters(name=adapter_name, enabled=True)  # enable only the current adapter we want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.freeze()\n",
    "# model.unfreeze_enabled_adapters()\n",
    "# #model.unfreeze()\n",
    "# model.decoder.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare NeMo's Experiment manager to handle checkpoint saving and logging for us\n",
    "from nemo.utils import exp_manager\n",
    "\n",
    "# Environment variable generally used for multi-node multi-gpu training.\n",
    "# In notebook environments, this flag is unnecessary and can cause logs of multiple training runs to overwrite each other.\n",
    "os.environ.pop('NEMO_EXPM_VERSION', None)\n",
    "\n",
    "exp_config = exp_manager.ExpManagerConfig(\n",
    "    exp_dir=f'/external2/karan_exp/experiments/',\n",
    "    name=f\"finetune-multidomain-alllang-100k-ctc\",\n",
    "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
    "        monitor=\"val_wer\",\n",
    "        mode=\"min\",\n",
    "        always_save_nemo=True,\n",
    "        save_best_model=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "exp_config = OmegaConf.structured(exp_config)\n",
    "\n",
    "logdir = exp_manager.exp_manager(trainer, exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cfg['tokenizer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, train the adapters\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
