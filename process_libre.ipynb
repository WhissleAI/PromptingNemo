{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from pathlib import PurePath\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intitiate text normalizer and puctuator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:48:21 cloud:58] Found existing object /home/ubuntu/.cache/torch/NeMo/NeMo_1.21.0rc0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo.\n",
      "[NeMo I 2023-11-12 07:48:21 cloud:64] Re-using file from: /home/ubuntu/.cache/torch/NeMo/NeMo_1.21.0rc0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo\n",
      "[NeMo I 2023-11-12 07:48:21 common:913] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2023-11-12 07:48:23 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: distilbert-base-uncased, vocab_file: /tmp/tmpgvak8g7y/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-12 07:48:23 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2023-11-12 07:48:23 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-12 07:48:23 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-12 07:48:23 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2023-11-12 07:48:23 nlp_overrides:438] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
      "    Megatron-based models require Apex to function correctly.\n",
      "[NeMo W 2023-11-12 07:48:23 nlp_overrides:446] megatron-core was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/NeMo#megatron-gpt.\n",
      "[NeMo W 2023-11-12 07:48:25 punctuation_capitalization_model:719] The artifact `class_labels.punct_labels_file` was not found in checkpoint. Will rely on `punct_label_ids` parameter\n",
      "[NeMo W 2023-11-12 07:48:25 punctuation_capitalization_model:741] The artifact `class_labels.capit_labels_file` was not found in checkpoint. Will rely on `capit_label_ids` parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:48:25 save_restore_connector:249] Model PunctuationCapitalizationModel was successfully restored from /home/ubuntu/.cache/torch/NeMo/NeMo_1.21.0rc0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo.\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer(input_case='lower_cased', lang=\"en\")\n",
    "punctuator = nemo_nlp.models.PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_distilbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Hugging Face NLP systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-12 07:49:51 nemo_logging:349] /home/ubuntu/anaconda3/envs/nemo/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "entity_tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "entity_model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "hf_nlp = pipeline(\"ner\", model=entity_model, tokenizer=entity_tokenizer, grouped_entities=True)\n",
    "\n",
    "\n",
    "def tag_entities(text):\n",
    "\n",
    "    ner_results = hf_nlp(text)\n",
    "    print(ner_results)\n",
    "\n",
    "    # example: [{'entity_group': 'PER', 'score': 0.8913538, 'word': 'Min', 'start': 0, 'end': 3}, {'entity_group': 'LOC', 'score': 0.9983326, 'word': 'West Van Buren Street', 'start': 93, 'end': 114}]\n",
    "    for ner_dict in ner_results:\n",
    "\n",
    "        entity_group = ner_dict['entity_group']\n",
    "        start = ner_dict['start']\n",
    "        end = ner_dict['end']\n",
    "        word = ner_dict['word']\n",
    "\n",
    "        text = text.replace(word, \"B-\"+entity_group+\" \"+word+\" E-\"+entity_group)\n",
    "\n",
    "    print(\"ner tagged text\", text)\n",
    "\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start pretrained Emotion Classification system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rajaram1996/Hubert_emotion were not used when initializing HubertForSpeechClassification: ['hubert.encoder.pos_conv_embed.conv.weight_v', 'hubert.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing HubertForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSpeechClassification were not initialized from the model checkpoint at Rajaram1996/Hubert_emotion and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor\n",
    "from src.models import Wav2Vec2ForSpeechClassification, HubertForSpeechClassification\n",
    "\n",
    "emotion_model = HubertForSpeechClassification.from_pretrained(\"Rajaram1996/Hubert_emotion\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "sampling_rate=16000 # defined by the model; must convert mp3 to this rate.\n",
    "config = AutoConfig.from_pretrained(\"Rajaram1996/Hubert_emotion\")\n",
    "\n",
    "def speech_file_to_array_fn(path, sampling_rate):\n",
    "    speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(_sampling_rate, sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "\n",
    "def predict(path, sampling_rate):\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in\n",
    "               enumerate(scores)]\n",
    "    return outputs\n",
    "\n",
    "def get_emotion_labels(audio_file, sampling_rate=16000, score=50.0):\n",
    "    sound_array = speech_file_to_array_fn(audio_file, sampling_rate)\n",
    "    \n",
    "    inputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(\"cpu\").float() for key in inputs}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "    outputs = [{\n",
    "        \"emo\": config.id2label[i],\n",
    "        \"score\": round(score * 100, 1)}\n",
    "        for i, score in enumerate(scores)\n",
    "    ]\n",
    "\n",
    "    #[{'emo': 'female_neutral', 'score': 73.9}, {'emo': 'female_happy', 'score': 24.8}]\n",
    "    emotion_labels = [row for row in sorted(outputs, key=lambda x:x[\"score\"], reverse=True) if row['score'] != '0.0%'][:2]\n",
    "\n",
    "    all_labels = []\n",
    "    for emotion_dict in emotion_labels:\n",
    "        label = emotion_dict['emo'].split(\"_\")[1].upper()\n",
    "        score = emotion_dict['score']\n",
    "\n",
    "        if score > 50.0:\n",
    "            all_labels.append(label)\n",
    "\n",
    "    return all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librespeech: Get data, un-compress it and then set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar (child): train.tar.gz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n",
      "tar (child): test.tar.gz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n",
      "tar (child): dev.tar.gz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unzip downloaded files\n",
    "os.system(\"tar -xzvf train.tar.gz\")\n",
    "os.system(\"tar -xzvf test.tar.gz\")\n",
    "os.system(\"tar -xzvf dev.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths to folders created afte unzipping\n",
    "LIBRE = '/n/disk1/audio_datasets/EN_libre/'\n",
    "TRAIN_DATA = Path(LIBRE+'/LibriSpeech/train-clean-360/')\n",
    "TRAIN_DATA_WAV = str(TRAIN_DATA) + '-wav/'\n",
    "os.system('mkdir -p ' + TRAIN_DATA_WAV)\n",
    "TRAIN_DATA_WAV = Path(TRAIN_DATA_WAV)\n",
    "\n",
    "\n",
    "DEV_DATA = Path(LIBRE+'/LibriSpeech/dev-clean/')\n",
    "DEV_DATA_WAV = str(DEV_DATA) + '-wav/'\n",
    "os.system('mkdir -p ' + DEV_DATA_WAV)\n",
    "DEV_DATA_WAV = Path(DEV_DATA_WAV)\n",
    "\n",
    "TEST_DATA = Path(LIBRE+'/LibriSpeech/test-clean/')\n",
    "TEST_DATA_WAV = str(TEST_DATA) + '-wav/'\n",
    "os.system('mkdir -p ' + TEST_DATA_WAV)\n",
    "TEST_DATA_WAV = Path(TEST_DATA_WAV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"was\" } tokens { name: \"in\" } tokens { name: \"a\" } tokens { name: \"fevered\" } tokens { name: \"state\" } tokens { name: \"of\" } tokens { name: \"mind\" } tokens { name: \"owing\" } tokens { name: \"to\" } tokens { name: \"the\" } tokens { name: \"blight\" } tokens { name: \"his\" } tokens { name: \"wife's\" } tokens { name: \"action\" } tokens { name: \"threatened\" } tokens { name: \"to\" } tokens { name: \"cast\" } tokens { name: \"upon\" } tokens { name: \"his\" } tokens { name: \"entire\" } tokens { name: \"future\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean\n",
      "/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean\n",
      "['/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2035', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2086', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/7976', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/1988', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/777', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/84', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/1673', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/8297', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/5536', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/6345', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/6295', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/6313', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/5338', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/6319', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/1993', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2078', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/1272', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/174', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2803', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/7850', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2902', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/3752', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/3081', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/1462', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/251', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2428', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/3170', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/3000', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/5895', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/5694', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2412', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/3536', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/3576', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/6241', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/652', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/422', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/8842', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/1919', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/3853']\n",
      "ss /n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896\n",
      "segments ['/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0026.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0005.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0033.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0006.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0018.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0034.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0021.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0015.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0012.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0027.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0007.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0030.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0011.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0009.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896.trans.txt', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0003.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0004.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0017.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0031.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0002.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0013.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0024.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0028.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0023.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0000.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0010.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0001.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0019.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0008.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0032.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0016.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0014.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0022.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0025.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0029.flac', '/n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0020.flac']\n",
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_infer_dataset:127] Max length: 28\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:406] Min: 26 |                  Max: 26 |                  Mean: 26.0 |                  Median: 26.0\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:412] 75 percentile: 26.00\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:413] 99 percentile: 26.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.50batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"would\" } tokens { name: \"have\" } tokens { name: \"to\" } tokens { name: \"pay\" } tokens { name: \"her\" } tokens { name: \"the\" } tokens { name: \"money\" } tokens { name: \"which\" } tokens { name: \"she\" } tokens { name: \"would\" } tokens { name: \"now\" } tokens { name: \"regularly\" } tokens { name: \"demand\" } tokens { name: \"or\" } tokens { name: \"there\" } tokens { name: \"would\" } tokens { name: \"be\" } tokens { name: \"trouble\" } tokens { name: \"it\" } tokens { name: \"did\" } tokens { name: \"not\" } tokens { name: \"matter\" } tokens { name: \"what\" } tokens { name: \"he\" } tokens { name: \"did\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_infer_dataset:127] Max length: 28\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:406] Min: 26 |                  Max: 26 |                  Mean: 26.0 |                  Median: 26.0\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:412] 75 percentile: 26.00\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:413] 99 percentile: 26.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.67batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"hurstwood\" } tokens { name: \"walked\" } tokens { name: \"the\" } tokens { name: \"floor\" } tokens { name: \"mentally\" } tokens { name: \"arranging\" } tokens { name: \"the\" } tokens { name: \"chief\" } tokens { name: \"points\" } tokens { name: \"of\" } tokens { name: \"his\" } tokens { name: \"situation\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_infer_dataset:127] Max length: 15\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:406] Min: 13 |                  Max: 13 |                  Mean: 13.0 |                  Median: 13.0\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:412] 75 percentile: 13.00\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:413] 99 percentile: 13.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.84batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"also\" } tokens { name: \"thought\" } tokens { name: \"of\" } tokens { name: \"his\" } tokens { name: \"managerial\" } tokens { name: \"position\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_infer_dataset:127] Max length: 9\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:406] Min: 7 |                  Max: 7 |                  Mean: 7.0 |                  Median: 7.0\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:412] 75 percentile: 7.00\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:413] 99 percentile: 7.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.14batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"how\" } tokens { name: \"would\" } tokens { name: \"the\" } tokens { name: \"papers\" } tokens { name: \"talk\" } tokens { name: \"about\" } tokens { name: \"it\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_infer_dataset:127] Max length: 9\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:406] Min: 7 |                  Max: 7 |                  Mean: 7.0 |                  Median: 7.0\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:412] 75 percentile: 7.00\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:413] 99 percentile: 7.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.26batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"many\" } tokens { name: \"little\" } tokens { name: \"wrinkles\" } tokens { name: \"gathered\" } tokens { name: \"between\" } tokens { name: \"his\" } tokens { name: \"eyes\" } tokens { name: \"as\" } tokens { name: \"he\" } tokens { name: \"contemplated\" } tokens { name: \"this\" } tokens { name: \"and\" } tokens { name: \"his\" } tokens { name: \"brow\" } tokens { name: \"moistened\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_infer_dataset:127] Max length: 19\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:406] Min: 17 |                  Max: 17 |                  Mean: 17.0 |                  Median: 17.0\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:412] 75 percentile: 17.00\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:413] 99 percentile: 17.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.15batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"could\" } tokens { name: \"arrange\" } tokens { name: \"that\" } tokens { name: \"satisfactorily\" } tokens { name: \"for\" } tokens { name: \"carrie\" } tokens { name: \"would\" } tokens { name: \"be\" } tokens { name: \"glad\" } tokens { name: \"to\" } tokens { name: \"wait\" } tokens { name: \"if\" } tokens { name: \"necessary\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:48 punctuation_capitalization_infer_dataset:127] Max length: 20\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:406] Min: 18 |                  Max: 18 |                  Mean: 18.0 |                  Median: 18.0\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:412] 75 percentile: 18.00\n",
      "[NeMo I 2023-11-12 07:58:48 data_preprocessing:413] 99 percentile: 18.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.39batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"would\" } tokens { name: \"see\" } tokens { name: \"how\" } tokens { name: \"things\" } tokens { name: \"turned\" } tokens { name: \"out\" } tokens { name: \"to\" } tokens { name: \"morrow\" } tokens { name: \"and\" } tokens { name: \"then\" } tokens { name: \"he\" } tokens { name: \"would\" } tokens { name: \"talk\" } tokens { name: \"to\" } tokens { name: \"her\" } tokens { name: \"they\" } tokens { name: \"were\" } tokens { name: \"going\" } tokens { name: \"to\" } tokens { name: \"meet\" } tokens { name: \"as\" } tokens { name: \"usual\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_infer_dataset:127] Max length: 25\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:406] Min: 23 |                  Max: 23 |                  Mean: 23.0 |                  Median: 23.0\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:412] 75 percentile: 23.00\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:413] 99 percentile: 23.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.34batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"for\" } tokens { name: \"some\" } tokens { name: \"reason\" } tokens { name: \"he\" } tokens { name: \"felt\" } tokens { name: \"as\" } tokens { name: \"if\" } tokens { name: \"something\" } tokens { name: \"might\" } tokens { name: \"come\" } tokens { name: \"that\" } tokens { name: \"way\" } tokens { name: \"and\" } tokens { name: \"was\" } tokens { name: \"relieved\" } tokens { name: \"when\" } tokens { name: \"all\" } tokens { name: \"the\" } tokens { name: \"envelopes\" } tokens { name: \"had\" } tokens { name: \"been\" } tokens { name: \"scanned\" } tokens { name: \"and\" } tokens { name: \"nothing\" } tokens { name: \"suspicious\" } tokens { name: \"noticed\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_infer_dataset:127] Max length: 29\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:406] Min: 27 |                  Max: 27 |                  Mean: 27.0 |                  Median: 27.0\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:412] 75 percentile: 27.00\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:413] 99 percentile: 27.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.07batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"while\" } tokens { name: \"the\" } tokens { name: \"danger\" } tokens { name: \"had\" } tokens { name: \"not\" } tokens { name: \"lessened\" } tokens { name: \"it\" } tokens { name: \"had\" } tokens { name: \"not\" } tokens { name: \"as\" } tokens { name: \"yet\" } tokens { name: \"materialised\" } tokens { name: \"and\" } tokens { name: \"with\" } tokens { name: \"him\" } tokens { name: \"no\" } tokens { name: \"news\" } tokens { name: \"was\" } tokens { name: \"good\" } tokens { name: \"news\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_infer_dataset:127] Max length: 24\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:406] Min: 22 |                  Max: 22 |                  Mean: 22.0 |                  Median: 22.0\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:412] 75 percentile: 22.00\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:413] 99 percentile: 22.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.48batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"so\" } tokens { name: \"little\" } tokens { name: \"did\" } tokens { name: \"he\" } tokens { name: \"consider\" } tokens { name: \"drouet\" } tokens { name: \"that\" } tokens { name: \"it\" } tokens { name: \"never\" } tokens { name: \"once\" } tokens { name: \"occurred\" } tokens { name: \"to\" } tokens { name: \"him\" } tokens { name: \"to\" } tokens { name: \"worry\" } tokens { name: \"about\" } tokens { name: \"his\" } tokens { name: \"finding\" } tokens { name: \"out\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_infer_dataset:127] Max length: 23\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:406] Min: 21 |                  Max: 21 |                  Mean: 21.0 |                  Median: 21.0\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:412] 75 percentile: 21.00\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:413] 99 percentile: 21.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.07batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"grew\" } tokens { name: \"restless\" } tokens { name: \"as\" } tokens { name: \"he\" } tokens { name: \"ruminated\" } tokens { name: \"and\" } tokens { name: \"then\" } tokens { name: \"decided\" } tokens { name: \"that\" } tokens { name: \"perhaps\" } tokens { name: \"it\" } tokens { name: \"was\" } tokens { name: \"nothing\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_infer_dataset:127] Max length: 17\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:406] Min: 15 |                  Max: 15 |                  Mean: 15.0 |                  Median: 15.0\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:412] 75 percentile: 15.00\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:413] 99 percentile: 15.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.77batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"she\" } tokens { name: \"had\" } tokens { name: \"not\" } tokens { name: \"been\" } tokens { name: \"able\" } tokens { name: \"to\" } tokens { name: \"get\" } tokens { name: \"away\" } tokens { name: \"this\" } tokens { name: \"morning\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:49 punctuation_capitalization_infer_dataset:127] Max length: 12\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:406] Min: 10 |                  Max: 10 |                  Mean: 10.0 |                  Median: 10.0\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:412] 75 percentile: 10.00\n",
      "[NeMo I 2023-11-12 07:58:49 data_preprocessing:413] 99 percentile: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.07batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"would\" } tokens { name: \"get\" } tokens { name: \"one\" } tokens { name: \"to\" } tokens { name: \"day\" } tokens { name: \"it\" } tokens { name: \"would\" } tokens { name: \"probably\" } tokens { name: \"be\" } tokens { name: \"on\" } tokens { name: \"his\" } tokens { name: \"desk\" } tokens { name: \"when\" } tokens { name: \"he\" } tokens { name: \"got\" } tokens { name: \"back\" } tokens { name: \"he\" } tokens { name: \"would\" } tokens { name: \"look\" } tokens { name: \"for\" } tokens { name: \"it\" } tokens { name: \"at\" } tokens { name: \"once\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_infer_dataset:127] Max length: 26\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:406] Min: 24 |                  Max: 24 |                  Mean: 24.0 |                  Median: 24.0\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:412] 75 percentile: 24.00\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:413] 99 percentile: 24.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.16batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"after\" } tokens { name: \"a\" } tokens { name: \"time\" } tokens { name: \"he\" } tokens { name: \"gave\" } tokens { name: \"up\" } tokens { name: \"waiting\" } tokens { name: \"and\" } tokens { name: \"drearily\" } tokens { name: \"headed\" } tokens { name: \"for\" } tokens { name: \"the\" } tokens { name: \"madison\" } tokens { name: \"car\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_infer_dataset:127] Max length: 18\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:406] Min: 16 |                  Max: 16 |                  Mean: 16.0 |                  Median: 16.0\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:412] 75 percentile: 16.00\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:413] 99 percentile: 16.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.31batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"went\" } tokens { name: \"in\" } tokens { name: \"and\" } tokens { name: \"examined\" } tokens { name: \"his\" } tokens { name: \"letters\" } tokens { name: \"but\" } tokens { name: \"there\" } tokens { name: \"was\" } tokens { name: \"nothing\" } tokens { name: \"from\" } tokens { name: \"carrie\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_infer_dataset:127] Max length: 15\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:406] Min: 13 |                  Max: 13 |                  Mean: 13.0 |                  Median: 13.0\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:412] 75 percentile: 13.00\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:413] 99 percentile: 13.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.78batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"fortunately\" } tokens { name: \"there\" } tokens { name: \"was\" } tokens { name: \"nothing\" } tokens { name: \"from\" } tokens { name: \"his\" } tokens { name: \"wife\" } tokens { name: \"either\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_infer_dataset:127] Max length: 10\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:406] Min: 8 |                  Max: 8 |                  Mean: 8.0 |                  Median: 8.0\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:412] 75 percentile: 8.00\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:413] 99 percentile: 8.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.94batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"at\" } tokens { name: \"one\" } tokens { name: \"thirty\" } tokens { name: \"he\" } tokens { name: \"went\" } tokens { name: \"to\" } tokens { name: \"rector's\" } tokens { name: \"for\" } tokens { name: \"lunch\" } tokens { name: \"and\" } tokens { name: \"when\" } tokens { name: \"he\" } tokens { name: \"returned\" } tokens { name: \"a\" } tokens { name: \"messenger\" } tokens { name: \"was\" } tokens { name: \"waiting\" } tokens { name: \"for\" } tokens { name: \"him\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_infer_dataset:127] Max length: 23\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:406] Min: 21 |                  Max: 21 |                  Mean: 21.0 |                  Median: 21.0\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:412] 75 percentile: 21.00\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:413] 99 percentile: 21.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.69batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"his\" } tokens { name: \"first\" } tokens { name: \"impulse\" } tokens { name: \"was\" } tokens { name: \"to\" } tokens { name: \"write\" } tokens { name: \"but\" } tokens { name: \"four\" } tokens { name: \"words\" } tokens { name: \"in\" } tokens { name: \"reply\" } tokens { name: \"go\" } tokens { name: \"to\" } tokens { name: \"the\" } tokens { name: \"devil\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_infer_dataset:127] Max length: 17\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:406] Min: 15 |                  Max: 15 |                  Mean: 15.0 |                  Median: 15.0\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:412] 75 percentile: 15.00\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:413] 99 percentile: 15.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.34batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"but\" } tokens { name: \"he\" } tokens { name: \"compromised\" } tokens { name: \"by\" } tokens { name: \"telling\" } tokens { name: \"the\" } tokens { name: \"boy\" } tokens { name: \"that\" } tokens { name: \"there\" } tokens { name: \"would\" } tokens { name: \"be\" } tokens { name: \"no\" } tokens { name: \"reply\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:50 punctuation_capitalization_infer_dataset:127] Max length: 15\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:406] Min: 13 |                  Max: 13 |                  Mean: 13.0 |                  Median: 13.0\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:412] 75 percentile: 13.00\n",
      "[NeMo I 2023-11-12 07:58:50 data_preprocessing:413] 99 percentile: 13.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.14batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"then\" } tokens { name: \"he\" } tokens { name: \"sat\" } tokens { name: \"down\" } tokens { name: \"in\" } tokens { name: \"his\" } tokens { name: \"chair\" } tokens { name: \"and\" } tokens { name: \"gazed\" } tokens { name: \"without\" } tokens { name: \"seeing\" } tokens { name: \"contemplating\" } tokens { name: \"the\" } tokens { name: \"result\" } tokens { name: \"of\" } tokens { name: \"his\" } tokens { name: \"work\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_infer_dataset:127] Max length: 19\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:406] Min: 17 |                  Max: 17 |                  Mean: 17.0 |                  Median: 17.0\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:412] 75 percentile: 17.00\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:413] 99 percentile: 17.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.26batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"what\" } tokens { name: \"would\" } tokens { name: \"she\" } tokens { name: \"do\" } tokens { name: \"about\" } tokens { name: \"that\" } tokens { name: \"the\" } tokens { name: \"confounded\" } tokens { name: \"wretch\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_infer_dataset:127] Max length: 14\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:406] Min: 12 |                  Max: 12 |                  Mean: 12.0 |                  Median: 12.0\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:412] 75 percentile: 12.00\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:413] 99 percentile: 12.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.58batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"later\" } tokens { name: \"however\" } tokens { name: \"his\" } tokens { name: \"old\" } tokens { name: \"discretion\" } tokens { name: \"asserted\" } tokens { name: \"itself\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_infer_dataset:127] Max length: 9\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:406] Min: 7 |                  Max: 7 |                  Mean: 7.0 |                  Median: 7.0\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:412] 75 percentile: 7.00\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:413] 99 percentile: 7.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.16batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"something\" } tokens { name: \"had\" } tokens { name: \"to\" } tokens { name: \"be\" } tokens { name: \"done\" } tokens { name: \"a\" } tokens { name: \"climax\" } tokens { name: \"was\" } tokens { name: \"near\" } tokens { name: \"and\" } tokens { name: \"she\" } tokens { name: \"would\" } tokens { name: \"not\" } tokens { name: \"sit\" } tokens { name: \"idle\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_infer_dataset:127] Max length: 17\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:406] Min: 15 |                  Max: 15 |                  Mean: 15.0 |                  Median: 15.0\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:412] 75 percentile: 15.00\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:413] 99 percentile: 15.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.79batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"knew\" } tokens { name: \"her\" } tokens { name: \"well\" } tokens { name: \"enough\" } tokens { name: \"to\" } tokens { name: \"know\" } tokens { name: \"that\" } tokens { name: \"when\" } tokens { name: \"she\" } tokens { name: \"had\" } tokens { name: \"decided\" } tokens { name: \"upon\" } tokens { name: \"a\" } tokens { name: \"plan\" } tokens { name: \"she\" } tokens { name: \"would\" } tokens { name: \"follow\" } tokens { name: \"it\" } tokens { name: \"up\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_infer_dataset:127] Max length: 22\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:406] Min: 20 |                  Max: 20 |                  Mean: 20.0 |                  Median: 20.0\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:412] 75 percentile: 20.00\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:413] 99 percentile: 20.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.15batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"arose\" } tokens { name: \"from\" } tokens { name: \"his\" } tokens { name: \"chair\" } tokens { name: \"and\" } tokens { name: \"went\" } tokens { name: \"and\" } tokens { name: \"looked\" } tokens { name: \"out\" } tokens { name: \"into\" } tokens { name: \"the\" } tokens { name: \"street\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_infer_dataset:127] Max length: 15\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:406] Min: 13 |                  Max: 13 |                  Mean: 13.0 |                  Median: 13.0\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:412] 75 percentile: 13.00\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:413] 99 percentile: 13.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.40batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"the\" } tokens { name: \"long\" } tokens { name: \"drizzle\" } tokens { name: \"had\" } tokens { name: \"begun\" } tokens { name: \"pedestrians\" } tokens { name: \"had\" } tokens { name: \"turned\" } tokens { name: \"up\" } tokens { name: \"collars\" } tokens { name: \"and\" } tokens { name: \"trousers\" } tokens { name: \"at\" } tokens { name: \"the\" } tokens { name: \"bottom\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_infer_dataset:127] Max length: 20\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:406] Min: 18 |                  Max: 18 |                  Mean: 18.0 |                  Median: 18.0\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:412] 75 percentile: 18.00\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:413] 99 percentile: 18.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.12batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"hurstwood\" } tokens { name: \"almost\" } tokens { name: \"exclaimed\" } tokens { name: \"out\" } tokens { name: \"loud\" } tokens { name: \"at\" } tokens { name: \"the\" } tokens { name: \"insistency\" } tokens { name: \"of\" } tokens { name: \"this\" } tokens { name: \"thing\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:51 punctuation_capitalization_infer_dataset:127] Max length: 15\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:406] Min: 13 |                  Max: 13 |                  Mean: 13.0 |                  Median: 13.0\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:412] 75 percentile: 13.00\n",
      "[NeMo I 2023-11-12 07:58:51 data_preprocessing:413] 99 percentile: 13.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.64batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"put\" } tokens { name: \"on\" } tokens { name: \"his\" } tokens { name: \"hat\" } tokens { name: \"and\" } tokens { name: \"looked\" } tokens { name: \"around\" } tokens { name: \"for\" } tokens { name: \"his\" } tokens { name: \"umbrella\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_infer_dataset:127] Max length: 13\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:406] Min: 11 |                  Max: 11 |                  Mean: 11.0 |                  Median: 11.0\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:412] 75 percentile: 11.00\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:413] 99 percentile: 11.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.67batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"would\" } tokens { name: \"have\" } tokens { name: \"some\" } tokens { name: \"arrangement\" } tokens { name: \"of\" } tokens { name: \"this\" } tokens { name: \"thing\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_infer_dataset:127] Max length: 10\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:406] Min: 8 |                  Max: 8 |                  Mean: 8.0 |                  Median: 8.0\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:412] 75 percentile: 8.00\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:413] 99 percentile: 8.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.16batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"began\" } tokens { name: \"to\" } tokens { name: \"wish\" } tokens { name: \"that\" } tokens { name: \"he\" } tokens { name: \"had\" } tokens { name: \"compromised\" } tokens { name: \"in\" } tokens { name: \"some\" } tokens { name: \"way\" } tokens { name: \"or\" } tokens { name: \"other\" } tokens { name: \"that\" } tokens { name: \"he\" } tokens { name: \"had\" } tokens { name: \"sent\" } tokens { name: \"the\" } tokens { name: \"money\" } tokens { name: \"perhaps\" } tokens { name: \"he\" } tokens { name: \"could\" } tokens { name: \"do\" } tokens { name: \"it\" } tokens { name: \"up\" } tokens { name: \"here\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_infer_dataset:127] Max length: 28\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:406] Min: 26 |                  Max: 26 |                  Mean: 26.0 |                  Median: 26.0\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:412] 75 percentile: 26.00\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:413] 99 percentile: 26.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.26batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"would\" } tokens { name: \"go\" } tokens { name: \"in\" } tokens { name: \"and\" } tokens { name: \"see\" } tokens { name: \"anyhow\" } tokens { name: \"he\" } tokens { name: \"would\" } tokens { name: \"have\" } tokens { name: \"no\" } tokens { name: \"row\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_infer_dataset:127] Max length: 15\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:406] Min: 13 |                  Max: 13 |                  Mean: 13.0 |                  Median: 13.0\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:412] 75 percentile: 13.00\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:413] 99 percentile: 13.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.09batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"by\" } tokens { name: \"the\" } tokens { name: \"time\" } tokens { name: \"he\" } tokens { name: \"reached\" } tokens { name: \"his\" } tokens { name: \"own\" } tokens { name: \"street\" } tokens { name: \"he\" } tokens { name: \"was\" } tokens { name: \"keenly\" } tokens { name: \"alive\" } tokens { name: \"to\" } tokens { name: \"the\" } tokens { name: \"difficulties\" } tokens { name: \"of\" } tokens { name: \"his\" } tokens { name: \"situation\" } tokens { name: \"and\" } tokens { name: \"wished\" } tokens { name: \"over\" } tokens { name: \"and\" } tokens { name: \"over\" } tokens { name: \"that\" } tokens { name: \"some\" } tokens { name: \"solution\" } tokens { name: \"would\" } tokens { name: \"offer\" } tokens { name: \"itself\" } tokens { name: \"that\" } tokens { name: \"he\" } tokens { name: \"could\" } tokens { name: \"see\" } tokens { name: \"his\" } tokens { name: \"way\" } tokens { name: \"out\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_infer_dataset:127] Max length: 39\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:406] Min: 37 |                  Max: 37 |                  Mean: 37.0 |                  Median: 37.0\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:412] 75 percentile: 37.00\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:413] 99 percentile: 37.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.63batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"then\" } tokens { name: \"he\" } tokens { name: \"rang\" } tokens { name: \"the\" } tokens { name: \"bell\" } tokens { name: \"no\" } tokens { name: \"answer\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_infer_dataset:127] Max length: 9\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:406] Min: 7 |                  Max: 7 |                  Mean: 7.0 |                  Median: 7.0\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:412] 75 percentile: 7.00\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:413] 99 percentile: 7.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.71batch/s]\n",
      " NeMo-text-processing :: DEBUG    :: tokens { name: \"he\" } tokens { name: \"rang\" } tokens { name: \"again\" } tokens { name: \"this\" } tokens { name: \"time\" } tokens { name: \"harder\" } tokens { name: \"still\" } tokens { name: \"no\" } tokens { name: \"answer\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
      "[NeMo I 2023-11-12 07:58:52 punctuation_capitalization_infer_dataset:127] Max length: 11\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:406] Min: 9 |                  Max: 9 |                  Mean: 9.0 |                  Median: 9.0\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:412] 75 percentile: 9.00\n",
      "[NeMo I 2023-11-12 07:58:52 data_preprocessing:413] 99 percentile: 9.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old path /n/disk1/audio_datasets/EN_libre/LibriSpeech/dev-clean/2277/149896/2277-149896-0026.flac\n",
      "[]\n",
      "ner tagged text The long drizzle had begun, pedestrians had turned up collars and trousers at the bottom.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sammple_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/n/disk1/PromptingNemo/process_libre.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mfor\u001b[39;00m datakey \u001b[39min\u001b[39;00m allpath[\u001b[39m1\u001b[39m:\u001b[39m2\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     \u001b[39m#print(datapath)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     \u001b[39mprint\u001b[39m(datakey)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     process_librispeech(datakey)\n",
      "\u001b[1;32m/n/disk1/PromptingNemo/process_libre.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m tagged_transcription \u001b[39m=\u001b[39m tag_entities(transcription)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m sample_dict[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m tagged_transcription\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m sammple_dict[\u001b[39m'\u001b[39m\u001b[39minstruction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtranscribe and mark named entities\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m json\u001b[39m.\u001b[39mdump(sammple_dict, manifest)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baicrowd-cpu-node1/n/disk1/PromptingNemo/process_libre.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m manifest\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sammple_dict' is not defined"
     ]
    }
   ],
   "source": [
    "allpath = [TRAIN_DATA, DEV_DATA, TEST_DATA]\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    normalized = normalizer.normalize(text, verbose=True, punct_post_process=True)\n",
    "    normalized = [normalized]\n",
    "    norm_punctuated = punctuator.add_punctuation_capitalization(normalized)[0]\n",
    "    return norm_punctuated\n",
    "\n",
    "def read_transcription(filepath):\n",
    "\n",
    "    trans = open(filepath,'r').readlines()\n",
    "    trans_dict = {}\n",
    "    for line in trans:\n",
    "        line = line.strip().split()\n",
    "        text = ' '.join(line[1:])\n",
    "        text = normalize(text)\n",
    "\n",
    "        trans_dict[line[0]] = text\n",
    "    \n",
    "    return trans_dict\n",
    "\n",
    "def process_librispeech(datakey):\n",
    "\n",
    "    print(datakey)\n",
    "    datafolders = glob.glob(str(datakey)+'/*')\n",
    "    print(datafolders)\n",
    "\n",
    "    datakey_wav = str(datakey) + '-wav/'\n",
    "    os.system('mkdir -p ' + datakey_wav)\n",
    "    datakey_wav = Path(datakey_wav)\n",
    "\n",
    "    manifest = open(datakey_wav.name+'.json','w')\n",
    "\n",
    "    for folder in datafolders[:4]:\n",
    "        sessdirs = glob.glob(folder + '/*')\n",
    "\n",
    "        for sessdir in sessdirs:\n",
    "            print(\"ss\", sessdir)\n",
    "            segments = glob.glob(sessdir + '/*')\n",
    "            print(\"segments\", segments)\n",
    "\n",
    "            transcription = [x for x in segments if re.search(\".txt\", x)][0]\n",
    "            trans_dict = read_transcription(transcription)\n",
    "            #print(trans_dict)\n",
    "\n",
    "\n",
    "            for segment in segments:\n",
    "                #allfiles = glob.glob(segment + '/*')\n",
    "                \n",
    "                filepath = PurePath(segment)\n",
    "\n",
    "                if \".flac\" in str(filepath):\n",
    "                    \n",
    "                    sample_dict = {}\n",
    "\n",
    "                    print(\"Old path\", filepath)\n",
    "\n",
    "                    filekey = filepath.name.replace(filepath.suffix, \"\")\n",
    "                    transcription = trans_dict[filekey]\n",
    "                    wav_filepath = str(datakey_wav) + \"/\" + filekey + \".wav\"\n",
    "                    sample_dict['audiofilepath'] = wav_filepath\n",
    "                    sample_dict['text'] = transcription\n",
    "\n",
    "                    flac_tmp_audio_data = AudioSegment.from_file(filepath, filepath.suffix[1:])\n",
    "                    flac_tmp_audio_data.export(wav_filepath, format=\"wav\")\n",
    "                    sample_dict['instruction'] = \"transcribe speech\"\n",
    "\n",
    "                    json.dump(sample_dict, manifest)\n",
    "                    manifest.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    tagged_transcription = tag_entities(transcription)\n",
    "                    sample_dict['text'] = tagged_transcription\n",
    "                    sammple_dict['instruction'] = \"transcribe and mark named entities\"\n",
    "                    json.dump(sample_dict, manifest)\n",
    "                    manifest.write(\"\\n\")\n",
    "\n",
    "\n",
    "                    emotion_labels = get_emotion_labels(audio_file=wav_filepath, sampling_rate=16000)\n",
    "                    print(\"Emotion Labels\", emotion_labels)\n",
    "                    emotion_labels = ' '.join(emotion_labels)\n",
    "\n",
    "                    final_transcription = tagged_transcription + \" \" + emotion_labels\n",
    "\n",
    "                    sample_dict['text'] = final_transcription\n",
    "                    sammple_dict['instruction'] = \"transcribe, mark named entitites and track speaker emotion\"\n",
    "                    json.dump(sample_dict, manifest)\n",
    "                    manifest.write(\"\\n\")\n",
    "\n",
    "                    sample_dict['prompt'] = final_transcription\n",
    "                    print(\"tagged transcription\", tagged_transcription)\n",
    "    \n",
    "    manifest.close()\n",
    "\n",
    "for datakey in allpath[1:2]:\n",
    "    #print(datapath)\n",
    "    print(datakey)\n",
    "    process_librispeech(datakey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
