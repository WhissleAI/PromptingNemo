{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fe3870ae69489b816f38740347ab09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Use the token from the environment for authentication\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"  # Set HF_TOKEN if not already set\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "#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\n",
    "# Ask for the GitHub token\n",
    "#repo_url = 'https://github.com/username/repo.git'  # replace with your repo URL\n",
    "\n",
    "# Configure Git with your token\n",
    "os.environ['GITHUB_TOKEN'] = \"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\"\n",
    "!git config --global user.name \"ksingla025\"\n",
    "!git config --global user.email \"ksingla025@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (1.10.11)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.9.0)\n",
      "Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.11\n",
      "    Uninstalling pydantic-1.10.11:\n",
      "      Successfully uninstalled pydantic-1.10.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "johnsnowlabs 5.3.4 requires pydantic==1.10.11, but you have pydantic 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-2.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets\n",
    "!pip install -q pyspark==3.3.0 spark-nlp==3.2.0 pandas pydub\n",
    "!pip install -q --upgrade spark-nlp-display\n",
    "!pip install -q johnsnowlabs\n",
    "!pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = {\n",
    "    \"minds14\": {\"dataset_name\": \"PolyAI/minds14\", \"name\": \"en-US\", \"split\": \"train\"},\n",
    "    \"fluers\": {\"dataset_name\": \"google/fleurs\", \"name\": \"en_us\", \"split\": \"train\"},\n",
    "    \"ami\": {\"dataset_name\": \"esb/datasets\", \"split\": \"ami\"}\n",
    "}\n",
    "\n",
    "\n",
    "dataset_id = \"minds14\"\n",
    "\n",
    "mydataset = load_dataset(dataset_info[dataset_id][\"dataset_name\"], name = dataset_info[dataset_id][\"name\"], split = dataset_info[dataset_id][\"split\"])\n",
    "\n",
    "# #minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
    "# fluers = load_dataset(\"google/fleurs\", name=\"en_us\",split=\"train\")\n",
    "intent_names = mydataset.features[\"intent_class\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-09 11:45:16 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.23.0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo.\n",
      "[NeMo I 2024-05-09 11:45:16 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.23.0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo\n",
      "[NeMo I 2024-05-09 11:45:16 common:924] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-05-09 11:45:18 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: distilbert-base-uncased, vocab_file: /tmp/tmpo3r49dq8/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-09 11:45:18 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2024-05-09 11:45:18 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2024-05-09 11:45:18 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2024-05-09 11:45:18 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    use_audio: false\n",
      "    audio_file: null\n",
      "    sample_rate: 16000\n",
      "    use_bucketing: true\n",
      "    batch_size: 32\n",
      "    preload_audios: true\n",
      "    use_tarred_dataset: false\n",
      "    label_info_save_dir: null\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    tokens_in_batch: null\n",
      "    max_seq_length: 128\n",
      "    num_samples: -1\n",
      "    use_cache: true\n",
      "    cache_dir: null\n",
      "    get_label_frequences: false\n",
      "    verbose: true\n",
      "    n_jobs: 0\n",
      "    tar_metadata_file: null\n",
      "    tar_shuffle_n: 1\n",
      "    shard_strategy: scatter\n",
      "    shuffle: true\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    persistent_workers: true\n",
      "    ds_item: punct_dataset_complete\n",
      "    \n",
      "[NeMo W 2024-05-09 11:45:18 save_restore_connector:394] src path does not exist or it is not a path in nemo file. src value I got was: distilbert-base-uncased_encoder_config.json. Absolute: /workspace/PromptingNemo/distilbert-base-uncased_encoder_config.json\n",
      "[NeMo W 2024-05-09 11:45:18 save_restore_connector:394] src path does not exist or it is not a path in nemo file. src value I got was: punct_label_ids.csv. Absolute: /workspace/PromptingNemo/punct_label_ids.csv\n",
      "[NeMo W 2024-05-09 11:45:18 punctuation_capitalization_model:719] The artifact `class_labels.punct_labels_file` was not found in checkpoint. Will rely on `punct_label_ids` parameter\n",
      "[NeMo W 2024-05-09 11:45:18 save_restore_connector:394] src path does not exist or it is not a path in nemo file. src value I got was: capit_label_ids.csv. Absolute: /workspace/PromptingNemo/capit_label_ids.csv\n",
      "[NeMo W 2024-05-09 11:45:18 punctuation_capitalization_model:741] The artifact `class_labels.capit_labels_file` was not found in checkpoint. Will rely on `capit_label_ids` parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-09 11:45:18 save_restore_connector:249] Model PunctuationCapitalizationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/punctuation_en_distilbert/6bdea9786c4395fbbe02e4143d2e1cee/punctuation_en_distilbert.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0509 11:45:20.628112 134120535095104 tokenize_and_classify.py:86] Creating ClassifyFst grammars.\n"
     ]
    }
   ],
   "source": [
    "### Intitiate text normalizer and puctuator\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "#import nemo.collections.nlp as nemo_nlp\n",
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "\n",
    "# Initialize the punctuation capitalization model\n",
    "punctuator = PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_distilbert\")\n",
    "\n",
    "normalizer = Normalizer(input_case='lower_cased', lang=\"en\")\n",
    "#punctuator = nemo_nlp.models.PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_distilbert\")\n",
    "punctuator = punctuator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 51.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-09 11:23:35 punctuation_capitalization_infer_dataset:127] Max length: 9\n",
      "[NeMo I 2024-05-09 11:23:35 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-09 11:23:35 data_preprocessing:406] Min: 5 |                  Max: 7 |                  Mean: 6.0 |                  Median: 6.0\n",
      "[NeMo I 2024-05-09 11:23:35 data_preprocessing:412] 75 percentile: 7.00\n",
      "[NeMo I 2024-05-09 11:23:35 data_preprocessing:413] 99 percentile: 7.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.23batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Your text to normalize.',\n",
       " 'I am moving to London, Visit me.',\n",
       " 'Your text to normalize.',\n",
       " 'I am moving to London, Visit me.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(text, batch_size=32):\n",
    "\n",
    "    #text = text.lower()\n",
    "    normalized = normalizer.normalize_list(text, batch_size=batch_size)\n",
    "    #normalized = [normalized]\n",
    "    norm_punctuated = punctuator.add_punctuation_capitalization(normalized, batch_size=batch_size)\n",
    "\n",
    "    return norm_punctuated\n",
    "\n",
    "text_to_normalize = [\"Your text to normalize\",\"i am moving to london visit me\", \"Your text to normalize\",\"i am moving to london visit me\"]\n",
    "\n",
    "normalized_text = normalize(text_to_normalize, batch_size=32)\n",
    "normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting gliner\n",
      "  Downloading gliner-0.1.12-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gliner) (2.2.0a0+81ea7a4)\n",
      "Collecting transformers>=4.38.2 (from gliner)\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.21.4 (from gliner)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flair==0.13.1 (from gliner)\n",
      "  Downloading flair-0.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: scipy<=1.12 in /usr/local/lib/python3.10/dist-packages (from gliner) (1.12.0)\n",
      "Collecting seqeval (from gliner)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m354.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gliner) (4.66.1)\n",
      "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (1.34.50)\n",
      "Collecting bpemb>=0.3.2 (from flair==0.13.1->gliner)\n",
      "  Downloading bpemb-0.3.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting conllu>=4.0 (from flair==0.13.1->gliner)\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair==0.13.1->gliner)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (6.1.3)\n",
      "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (5.1.0)\n",
      "Collecting gensim>=4.2.0 (from flair==0.13.1->gliner)\n",
      "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting janome>=0.4.2 (from flair==0.13.1->gliner)\n",
      "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langdetect>=1.0.9 (from flair==0.13.1->gliner)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m178.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (5.1.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (3.8.2)\n",
      "Collecting more-itertools>=8.13.0 (from flair==0.13.1->gliner)\n",
      "  Downloading more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting mpld3>=0.3 (from flair==0.13.1->gliner)\n",
      "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair==0.13.1->gliner)\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (2.8.2)\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair==0.13.1->gliner)\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (2023.12.25)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (1.2.0)\n",
      "Collecting segtok>=1.5.11 (from flair==0.13.1->gliner)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair==0.13.1->gliner)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (0.9.0)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair==0.13.1->gliner)\n",
      "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from flair==0.13.1->gliner) (1.26.18)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair==0.13.1->gliner)\n",
      "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting semver<4.0.0,>=3.0.0 (from flair==0.13.1->gliner)\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.4->gliner) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.4->gliner) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.4->gliner) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.4->gliner) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.4->gliner) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.4->gliner) (4.9.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy<=1.12->gliner) (1.24.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gliner) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gliner) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gliner) (3.1.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.38.2->gliner)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->gliner) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.50 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair==0.13.1->gliner) (1.34.50)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair==0.13.1->gliner) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair==0.13.1->gliner) (0.10.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair==0.13.1->gliner) (0.2.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair==0.13.1->gliner) (1.16.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair==0.13.1->gliner) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair==0.13.1->gliner) (4.12.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair==0.13.1->gliner) (6.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect>=1.0.9->flair==0.13.1->gliner) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.13.1->gliner) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.13.1->gliner) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.13.1->gliner) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.13.1->gliner) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.13.1->gliner) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.13.1->gliner) (3.1.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair==0.13.1->gliner) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair==0.13.1->gliner) (3.2.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair==0.13.1->gliner) (4.24.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->gliner) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.4->gliner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.4->gliner) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.4->gliner) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->gliner) (1.3.0)\n",
      "Collecting accelerate>=0.21.0 (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair==0.13.1->gliner)\n",
      "  Downloading accelerate-0.30.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair==0.13.1->gliner) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair==0.13.1->gliner) (1.7.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair==0.13.1->gliner) (5.9.4)\n",
      "Downloading gliner-0.1.12-py3-none-any.whl (26 kB)\n",
      "Downloading flair-0.13.1-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m536.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m503.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m533.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
      "Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m316.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m233.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m377.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m536.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m348.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m528.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: seqeval, langdetect, pptree, sqlitedict\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=ccdbefcfe0e5207e9925a13d5873a8c2036bd2b7299d1688bc9b029ae90d720a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4praa0t/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=c05528340faa63ca238ae7037ee922b5829b1512b9a46c77fe11a21079c4f863\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4praa0t/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "  Building wheel for pptree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=8409a990903516767bea114b98cce4032114d0cfa8e400d829e69f77af0e1764\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4praa0t/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=92f5e1ccf34a182984c270fbeb252c2e11615d9ec494793edd53ad6f4b8c31e8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4praa0t/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
      "Successfully built seqeval langdetect pptree sqlitedict\n",
      "Installing collected packages: sqlitedict, pptree, janome, semver, segtok, more-itertools, langdetect, deprecated, conllu, wikipedia-api, huggingface-hub, gensim, tokenizers, seqeval, pytorch-revgrad, mpld3, bpemb, accelerate, transformers, transformer-smaller-training-vocab, flair, gliner\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.1\n",
      "    Uninstalling transformers-4.38.1:\n",
      "      Successfully uninstalled transformers-4.38.1\n",
      "Successfully installed accelerate-0.30.0 bpemb-0.3.5 conllu-4.5.3 deprecated-1.2.14 flair-0.13.1 gensim-4.3.2 gliner-0.1.12 huggingface-hub-0.23.0 janome-0.5.0 langdetect-1.0.9 more-itertools-10.2.0 mpld3-0.5.10 pptree-3.1 pytorch-revgrad-0.2.0 segtok-1.5.11 semver-3.0.2 seqeval-1.2.2 sqlitedict-2.1.0 tokenizers-0.19.1 transformer-smaller-training-vocab-0.4.0 transformers-4.40.2 wikipedia-api-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gliner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ronaldo => person\n",
      "Portuguese => nationality\n",
      "5 February 1985 => date\n",
      "Portuguese => nationality\n",
      "professional footballer => profession\n",
      "forward => profession\n",
      "Saudi Pro League club => organization\n",
      "Al Nassr => organization\n",
      "Portugal national team => organization\n",
      "Ronaldo => person\n",
      "five => quantity\n",
      "Ballon d'Or awards => award\n",
      "UEFA Men's Player of the Year Awards => award\n",
      "European Golden Shoes => award\n",
      "European => nationality\n",
      "five => quantity\n",
      "European Championship => event\n",
      "Ronaldo => person\n",
      "140 => quantity\n",
      "European Championship => event\n",
      "outfield player => profession\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_large-v2.1\")\n",
    "model = model.to(device)\n",
    "text = \"\"\"\n",
    "Cristiano Ronaldo dos Santos Aveiro (Portuguese pronunciation: [kɾiʃˈtjɐnu ʁɔˈnaldu]; born 5 February 1985) is a Portuguese professional footballer who plays as a forward for and captains both Saudi Pro League club Al Nassr and the Portugal national team. Widely regarded as one of the greatest players of all time, Ronaldo has won five Ballon d'Or awards,[note 3] a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship and the UEFA Nations League. Ronaldo holds the records for most appearances (183), goals (140) and assists (42) in the Champions League, goals in the European Championship (14), international goals (128) and international appearances (205). He is one of the few players to have made over 1,200 professional career appearances, the most by an outfield player, and has scored over 850 official senior career goals for club and country, making him the top goalscorer of all time.\n",
    "\"\"\"\n",
    "\n",
    "labels = [\n",
    "    \"person\",\n",
    "    \"organization\",\n",
    "    \"location\",\n",
    "    \"date\",\n",
    "    \"time\",\n",
    "    \"event\",\n",
    "    \"product\",\n",
    "    \"work of art\",\n",
    "    \"language\",\n",
    "    \"money\",\n",
    "    \"percentage\",\n",
    "    \"quantity\",\n",
    "    \"email\",\n",
    "    \"phone number\",\n",
    "    \"URL\",\n",
    "    \"username\",\n",
    "    \"hashtag\",\n",
    "    \"mention\",\n",
    "    \"emoji\",\n",
    "    \"acronym\",\n",
    "    \"abbreviation\",\n",
    "    \"title\",\n",
    "    \"gender\",\n",
    "    \"nationality\",\n",
    "    \"religion\",\n",
    "    \"ethnicity\",\n",
    "    \"political affiliation\",\n",
    "    \"profession\",\n",
    "    \"medical condition\",\n",
    "    \"food\",\n",
    "    \"sport\",\n",
    "    \"technology\",\n",
    "    \"currency\",\n",
    "    \"unit of measurement\",\n",
    "    \"vehicle\",\n",
    "    \"company\",\n",
    "    \"law\",\n",
    "    \"artifact\",\n",
    "    \"concept\",\n",
    "    \"number\",\n",
    "    \"ordinal\",\n",
    "    \"cardinal\",\n",
    "    \"symbol\",\n",
    "    \"game\",\n",
    "    \"award\",\n",
    "    \"book\",\n",
    "    \"movie\",\n",
    "    \"music\",\n",
    "    \"TV show\",\n",
    "    \"podcast\",\n",
    "    \"document\",\n",
    "    \"article\",\n",
    "    \"blog\",\n",
    "    \"social media\",\n",
    "    \"platform\",\n",
    "    \"network\",\n",
    "    \"software\",\n",
    "    \"hardware\",\n",
    "    \"operating system\",\n",
    "    \"framework\",\n",
    "    \"library\",\n",
    "    \"database\",\n",
    "    \"algorithm\",\n",
    "    \"encryption\",\n",
    "    \"authentication\",\n",
    "    \"authorization\",\n",
    "    \"cybersecurity\",\n",
    "    \"privacy\",\n",
    "    \"hacking\",\n",
    "    \"malware\",\n",
    "    \"phishing\",\n",
    "    \"spam\",\n",
    "    \"virus\",\n",
    "    \"trojan\",\n",
    "    \"worm\",\n",
    "    \"firewall\",\n",
    "    \"backup\",\n",
    "    \"cloud computing\",\n",
    "    \"big data\",\n",
    "    \"artificial intelligence\",\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"neural network\",\n",
    "    \"natural language processing\",\n",
    "    \"computer vision\",\n",
    "    \"robotics\",\n",
    "    \"Internet of Things\",\n",
    "    \"blockchain\",\n",
    "    \"cryptocurrency\",\n",
    "    \"smart contract\",\n",
    "    \"decentralized finance\",\n",
    "    \"virtual reality\",\n",
    "    \"augmented reality\",\n",
    "    \"mixed reality\",\n",
    "    \"gaming\",\n",
    "    \"e-commerce\",\n",
    "    \"online shopping\",\n",
    "    \"digital marketing\",\n",
    "    \"social networking\",\n",
    "    \"streaming\",\n",
    "    \"video conferencing\",\n",
    "    \"remote work\",\n",
    "    \"collaboration\",\n",
    "    \"education\",\n",
    "    \"e-learning\",\n",
    "    \"training\",\n",
    "    \"remote learning\",\n",
    "    \"healthcare\",\n",
    "    \"telemedicine\",\n",
    "    \"fitness\",\n",
    "    \"wellness\",\n",
    "    \"mental health\",\n",
    "    \"environment\",\n",
    "    \"sustainability\",\n",
    "    \"climate change\",\n",
    "    \"renewable energy\",\n",
    "    \"green technology\",\n",
    "    \"biodiversity\",\n",
    "    \"conservation\",\n",
    "    \"pollution\",\n",
    "    \"oceanography\",\n",
    "    \"geology\",\n",
    "    \"meteorology\",\n",
    "    \"astronomy\",\n",
    "    \"physics\",\n",
    "    \"chemistry\",\n",
    "    \"biology\",\n",
    "    \"genetics\",\n",
    "    \"ecology\",\n",
    "    \"evolution\",\n",
    "    \"anthropology\",\n",
    "    \"psychology\",\n",
    "    \"sociology\",\n",
    "    \"economics\",\n",
    "    \"finance\",\n",
    "    \"business\",\n",
    "    \"management\",\n",
    "    \"leadership\",\n",
    "    \"entrepreneurship\",\n",
    "    \"startups\",\n",
    "    \"venture capital\",\n",
    "    \"angel investing\",\n",
    "    \"private equity\",\n",
    "    \"stock market\",\n",
    "    \"real estate\",\n",
    "    \"insurance\",\n",
    "    \"tax\",\n",
    "    \"accounting\",\n",
    "    \"audit\",\n",
    "    \"consulting\",\n",
    "    \"legal\",\n",
    "    \"human resources\",\n",
    "    \"marketing\",\n",
    "    \"sales\",\n",
    "    \"customer service\",\n",
    "    \"product management\",\n",
    "    \"project management\",\n",
    "    \"quality management\",\n",
    "    \"supply chain\",\n",
    "    \"logistics\",\n",
    "    \"operations\",\n",
    "    \"manufacturing\",\n",
    "    \"distribution\",\n",
    "    \"retail\",\n",
    "    \"hospitality\",\n",
    "    \"tourism\",\n",
    "    \"travel\",\n",
    "    \"transportation\",\n",
    "    \"automotive\",\n",
    "    \"aviation\",\n",
    "    \"railway\",\n",
    "    \"maritime\",\n",
    "    \"construction\",\n",
    "    \"architecture\",\n",
    "    \"engineering\",\n",
    "    \"design\",\n",
    "    \"art\",\n",
    "    \"music\",\n",
    "    \"literature\",\n",
    "    \"film\",\n",
    "    \"theater\",\n",
    "    \"dance\",\n",
    "    \"photography\",\n",
    "    \"painting\",\n",
    "    \"sculpture\",\n",
    "    \"crafts\",\n",
    "    \"fashion\",\n",
    "    \"culinary arts\",\n",
    "    \"beverage\",\n",
    "    \"wine\",\n",
    "    \"beer\",\n",
    "    \"spirits\",\n",
    "    \"cocktail\",\n",
    "    \"coffee\",\n",
    "    \"tea\",\n",
    "    \"soft drink\",\n",
    "    \"juice\",\n",
    "    \"water\",\n",
    "    \"energy drink\",\n",
    "    \"sports drink\",\n",
    "    \"alcohol\",\n",
    "    \"liquor\",\n",
    "    \"whiskey\",\n",
    "    \"vodka\",\n",
    "    \"rum\",\n",
    "    \"gin\",\n",
    "    \"tequila\",\n",
    "    \"brandy\",\n",
    "    \"cognac\",\n",
    "    \"champagne\",\n",
    "    \"wine varietal\",\n",
    "    \"craft beer\",\n",
    "    \"microbrewery\",\n",
    "    \"homebrewing\",\n",
    "    \"bartending\",\n",
    "    \"mixology\",\n",
    "    \"food pairing\",\n",
    "    \"food presentation\",\n",
    "    \"gastronomy\",\n",
    "    \"culinary tourism\",\n",
    "    \"food blogging\",\n",
    "    \"restaurant\",\n",
    "    \"cafe\",\n",
    "    \"bar\",\n",
    "    \"pub\",\n",
    "    \"bistro\",\n",
    "    \"diner\",\n",
    "    \"food truck\",\n",
    "    \"street food\",\n",
    "    \"fast food\",\n",
    "    \"casual dining\",\n",
    "    \"fine dining\",\n",
    "    \"takeout\",\n",
    "    \"delivery\",\n",
    "    \"cuisine\",\n",
    "    \"regional cuisine\",\n",
    "    \"ethnic cuisine\",\n",
    "    \"fusion cuisine\",\n",
    "    \"gourmet cuisine\",\n",
    "    \"organic\",\n",
    "    \"sustainable\",\n",
    "    \"local\",\n",
    "    \"farm-to-table\",\n",
    "    \"seasonal\",\n",
    "    \"vegetarian\",\n",
    "    \"vegan\",\n",
    "    \"gluten-free\",\n",
    "    \"allergy-friendly\",\n",
    "    \"paleo\",\n",
    "    \"keto\",\n",
    "    \"intermittent fasting\",\n",
    "    \"flexitarian\",\n",
    "    \"plant-based\",\n",
    "    \"meatless\",\n",
    "    \"meat substitute\",\n",
    "    \"plant-based protein\",\n",
    "    \"nutrient-dense\",\n",
    "    \"functional food\",\n",
    "    \"superfood\",\n",
    "    \"antioxidant\",\n",
    "    \"vitamin\",\n",
    "    \"mineral\",\n",
    "    \"protein\",\n",
    "    \"carbohydrate\",\n",
    "    \"fat\",\n",
    "    \"fiber\",\n",
    "    \"calorie\",\n",
    "    \"sugar\",\n",
    "    \"salt\",\n",
    "    \"sodium\",\n",
    "    \"cholesterol\",\n",
    "    \"caffeine\",\n",
    "    \"hydration\",\n",
    "    \"water intake\",\n",
    "    \"exercise\",\n",
    "    \"cardio\",\n",
    "    \"strength training\",\n",
    "    \"resistance training\",\n",
    "    \"flexibility\",\n",
    "    \"mobility\",\n",
    "    \"balance\",\n",
    "    \"endurance\",\n",
    "    \"agility\",\n",
    "    \"speed\",\n",
    "    \"power\",\n",
    "    \"stamina\",\n",
    "    \"anaerobic\",\n",
    "    \"aerobic\",\n",
    "    \"HIIT\",\n",
    "    \"yoga\",\n",
    "    \"pilates\",\n",
    "    \"barre\",\n",
    "    \"stretching\",\n",
    "    \"functional movement\",\n",
    "    \"calisthenics\",\n",
    "    \"bodyweight exercise\",\n",
    "    \"weightlifting\",\n",
    "    \"bodybuilding\",\n",
    "    \"crossfit\",\n",
    "    \"sports performance\",\n",
    "    \"athletics\",\n",
    "    \"running\",\n",
    "    \"jogging\",\n",
    "    \"walking\",\n",
    "    \"cycling\",\n",
    "    \"swimming\",\n",
    "    \"rowing\",\n",
    "    \"skiing\",\n",
    "    \"snowboarding\",\n",
    "    \"skateboarding\",\n",
    "    \"surfing\",\n",
    "    \"climbing\",\n",
    "    \"hiking\",\n",
    "    \"camping\",\n",
    "    \"outdoor recreation\",\n",
    "    \"adventure travel\",\n",
    "    \"sightseeing\",\n",
    "    \"museum\",\n",
    "    \"art gallery\",\n",
    "    \"historic site\",\n",
    "    \"landmark\",\n",
    "    \"architecture\",\n",
    "    \"scenic view\",\n",
    "    \"nature reserve\",\n",
    "    \"wildlife sanctuary\",\n",
    "    \"park\",\n",
    "    \"botanical garden\",\n",
    "    \"zoo\",\n",
    "    \"aquarium\",\n",
    "    \"theme park\",\n",
    "    \"amusement park\",\n",
    "    \"water park\",\n",
    "    \"recreational facility\",\n",
    "    \"sports stadium\",\n",
    "    \"arena\",\n",
    "    \"concert hall\",\n",
    "    \"theater\",\n",
    "    \"cinema\",\n",
    "    \"nightclub\",\n",
    "    \"pub\",\n",
    "    \"bar\",\n",
    "    \"cafe\",\n",
    "    \"restaurant\",\n",
    "    \"fast food\",\n",
    "    \"fine dining\",\n",
    "    \"cuisine\",\n",
    "    \"food\",\n",
    "    \"beverage\",\n",
    "    \"dessert\",\n",
    "    \"snack\",\n",
    "    \"coffee\",\n",
    "    \"tea\",\n",
    "    \"soft drink\",\n",
    "    \"juice\",\n",
    "    \"alcohol\",\n",
    "    \"cocktail\",\n",
    "    \"wine\",\n",
    "    \"beer\",\n",
    "    \"spirit\",\n",
    "    \"liquor\",\n",
    "    \"whiskey\",\n",
    "    \"vodka\",\n",
    "    \"rum\",\n",
    "    \"gin\",\n",
    "    \"tequila\",\n",
    "    \"brandy\",\n",
    "    \"cognac\",\n",
    "    \"champagne\",\n",
    "    \"wine varietal\",\n",
    "    \"craft beer\",\n",
    "    \"microbrewery\",\n",
    "    \"homebrewing\",\n",
    "    \"bartending\",\n",
    "    \"mixology\",\n",
    "    \"food pairing\",\n",
    "    \"food presentation\",\n",
    "    \"gastronomy\",\n",
    "    \"culinary tourism\",\n",
    "    \"food blogging\",\n",
    "    \"restaurant\",\n",
    "    \"cafe\",\n",
    "    \"bar\",\n",
    "    \"pub\",\n",
    "    \"bistro\",\n",
    "    \"diner\",\n",
    "    \"food truck\",\n",
    "    \"street food\",\n",
    "    \"fast food\",\n",
    "    \"casual dining\",\n",
    "    \"fine dining\",\n",
    "    \"takeout\",\n",
    "    \"delivery\",\n",
    "    \"cuisine\",\n",
    "    \"regional cuisine\",\n",
    "    \"ethnic cuisine\",\n",
    "    \"fusion cuisine\",\n",
    "    \"gourmet cuisine\",\n",
    "    \"organic\",\n",
    "    \"sustainable\",\n",
    "    \"local\",\n",
    "    \"farm-to-table\",\n",
    "    \"seasonal\",\n",
    "    \"vegetarian\",\n",
    "    \"vegan\",\n",
    "    \"gluten-free\",\n",
    "    \"allergy-friendly\",\n",
    "    \"paleo\",\n",
    "    \"keto\",\n",
    "    \"intermittent fasting\",\n",
    "    \"flexitarian\",\n",
    "    \"plant-based\",\n",
    "    \"meatless\",\n",
    "    \"meat substitute\",\n",
    "    \"plant-based protein\",\n",
    "    \"nutrient-dense\",\n",
    "    \"functional food\",\n",
    "    \"superfood\",\n",
    "    \"antioxidant\",\n",
    "    \"vitamin\",\n",
    "    \"mineral\",\n",
    "    \"protein\",\n",
    "    \"carbohydrate\",\n",
    "    \"fat\",\n",
    "    \"fiber\",\n",
    "    \"calorie\",\n",
    "    \"sugar\",\n",
    "    \"salt\",\n",
    "    \"sodium\",\n",
    "    \"cholesterol\",\n",
    "    \"caffeine\",\n",
    "    \"hydration\",\n",
    "    \"water intake\",\n",
    "    \"exercise\",\n",
    "    \"cardio\",\n",
    "    \"strength training\",\n",
    "    \"resistance training\",\n",
    "    \"flexibility\",\n",
    "    \"mobility\",\n",
    "    \"balance\",\n",
    "    \"endurance\",\n",
    "    \"agility\",\n",
    "    \"speed\",\n",
    "    \"power\",\n",
    "    \"stamina\",\n",
    "    \"anaerobic\",\n",
    "    \"aerobic\",\n",
    "    \"HIIT\",\n",
    "    \"yoga\",\n",
    "    \"pilates\",\n",
    "    \"barre\",\n",
    "    \"stretching\",\n",
    "    \"functional movement\",\n",
    "    \"calisthenics\",\n",
    "    \"bodyweight exercise\",\n",
    "    \"weightlifting\",\n",
    "    \"bodybuilding\",\n",
    "    \"crossfit\",\n",
    "    \"sports performance\",\n",
    "    \"athletics\",\n",
    "    \"running\",\n",
    "    \"jogging\",\n",
    "    \"walking\",\n",
    "    \"cycling\",\n",
    "    \"swimming\",\n",
    "    \"rowing\",\n",
    "    \"skiing\",\n",
    "    \"snowboarding\",\n",
    "    \"skateboarding\",\n",
    "    \"surfing\",\n",
    "    \"climbing\",\n",
    "    \"hiking\",\n",
    "    \"camping\",\n",
    "    \"outdoor recreation\",\n",
    "    \"adventure travel\",\n",
    "    \"sightseeing\",\n",
    "    \"museum\",\n",
    "    \"art gallery\",\n",
    "    \"historic site\",\n",
    "    \"landmark\",\n",
    "    \"architecture\",\n",
    "    \"scenic view\",\n",
    "    \"nature reserve\",\n",
    "    \"wildlife sanctuary\",\n",
    "    \"park\",\n",
    "    \"botanical garden\",\n",
    "    \"zoo\",\n",
    "    \"aquarium\",\n",
    "    \"theme park\",\n",
    "    \"amusement park\",\n",
    "    \"water park\",\n",
    "    \"recreational facility\",\n",
    "    \"sports stadium\",\n",
    "    \"arena\",\n",
    "    \"concert hall\",\n",
    "    \"theater\",\n",
    "    \"cinema\",\n",
    "    \"nightclub\",\n",
    "    \"pub\",\n",
    "    \"bar\",\n",
    "    \"cafe\",\n",
    "    \"restaurant\",\n",
    "    \"fast food\",\n",
    "    \"fine dining\",\n",
    "    \"cuisine\",\n",
    "    \"food\"]\n",
    "\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 1, 'end': 18, 'text': 'Cristiano Ronaldo', 'label': 'person', 'score': 0.8923930525779724}, {'start': 92, 'end': 107, 'text': '5 February 1985', 'label': 'date', 'score': 0.9758335947990417}, {'start': 233, 'end': 255, 'text': 'Portugal national team', 'label': 'teams', 'score': 0.7006853818893433}, {'start': 317, 'end': 324, 'text': 'Ronaldo', 'label': 'person', 'score': 0.5729483366012573}, {'start': 338, 'end': 356, 'text': \"Ballon d'Or awards\", 'label': 'award', 'score': 0.633683443069458}, {'start': 381, 'end': 417, 'text': \"UEFA Men's Player of the Year Awards\", 'label': 'award', 'score': 0.892940878868103}, {'start': 428, 'end': 449, 'text': 'European Golden Shoes', 'label': 'award', 'score': 0.8832414746284485}, {'start': 556, 'end': 578, 'text': 'UEFA Champions Leagues', 'label': 'competitions', 'score': 0.7971665859222412}, {'start': 584, 'end': 610, 'text': 'UEFA European Championship', 'label': 'competitions', 'score': 0.9217646718025208}, {'start': 619, 'end': 638, 'text': 'UEFA Nations League', 'label': 'competitions', 'score': 0.9559527635574341}, {'start': 640, 'end': 647, 'text': 'Ronaldo', 'label': 'person', 'score': 0.5068798661231995}, {'start': 761, 'end': 782, 'text': 'European Championship', 'label': 'competitions', 'score': 0.545802891254425}]\n"
     ]
    }
   ],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e001963727149cb87ef714d9d9fbb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-1.1-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me one word intent label for this sentence: I want to buy apples from the shop.\n",
      "\n",
      "The answer is transaction.\n",
      "\n",
      "Transaction is a word intent label that indicates the purpose or intention of the sentence. In this case, the sentence expresses the intention of purchasing apples from a shop.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Give me one word intent label for this sentence: I want to buy apples from the shop\"\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# Generate longer text sequence\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,  # Adjust the maximum length of the generated sequence\n",
    "    num_return_sequences=1,  # Generate only one sequence\n",
    "    temperature=0.1,  # Adjust the temperature parameter for diversity\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,   6571,    603,    573,   7201,    576, 206571, 235336,    109,\n",
      "            651,   2872,    603,   3482,    611,    476,  60193,  61023, 235265,\n",
      "           2456,    603]], device='cuda:0')\n",
      "<bos>Who is the king of bollywood?\n",
      "\n",
      "The question is based on a fictional premise. There is\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Who is the king of bollywood?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
