{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spm_model_path:  /home/svanga/test/model_shelf/EN_IoT_conformer_ctc_large/tokenizer/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "import wave\n",
    "import os\n",
    "\n",
    "\n",
    "# Configuration dictionary\n",
    "from utils.asr_utils import *\n",
    "\n",
    "\n",
    "# ort_session_en_ner, model_tokenizer_en, filterbank_featurizer = create_ort_session(model_name=\"EN_ner_emotion_commonvoice\", model_shelf=\"/disk1/artifacts/whissle/model_shelf\")\n",
    "ort_session_en_ner, model_tokenizer_en, filterbank_featurizer = create_ort_session(model_name=\"EN_ner_conformer_ctc_large\", model_shelf=\"/home/svanga/test/model_shelf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert stereo audio to mono\n",
    "def convert_to_stereo_mono(audio_path, output_path, channel=1):\n",
    "    stereo_audio = AudioSegment.from_wav(audio_path)\n",
    "    mono_audio = stereo_audio.set_channels(channel)\n",
    "    mono_audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n",
    "def calculate_n_buffers(audio_file_path, chunk_len_in_secs, buffer_len_in_secs, stride):\n",
    "    with wave.open(audio_file_path, 'r') as wav_file:\n",
    "        sample_rate = wav_file.getframerate()\n",
    "        num_samples = wav_file.getnframes()\n",
    "        duration_in_secs = num_samples / sample_rate\n",
    "\n",
    "    effective_chunk_len = chunk_len_in_secs + buffer_len_in_secs - stride\n",
    "\n",
    "    n_buffers = int(duration_in_secs / effective_chunk_len)\n",
    "\n",
    "    return n_buffers\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_mono(audio_path, output_path, channel=1):\n",
    "    # Load the stereo audio file\n",
    "    stereo_audio = AudioSegment.from_wav(audio_path)\n",
    "    \n",
    "    # Select the channel: 0 for left, 1 for right\n",
    "    if channel == 0:\n",
    "        mono_audio = stereo_audio.split_to_mono()[0]  # Left channel\n",
    "    elif channel == 1:\n",
    "        mono_audio = stereo_audio.split_to_mono()[1]  # Right channel\n",
    "    else:\n",
    "        raise ValueError(\"Channel must be 0 (left) or 1 (right)\")\n",
    "    \n",
    "    # Export the mono audio to the specified output path\n",
    "    mono_audio.export(output_path, format=\"wav\")\n",
    "    return output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple iterator class to return successive chunks of samples\n",
    "class AudioChunkIterator:\n",
    "    def __init__(self, samples, chunk_len_in_secs, sample_rate):\n",
    "        self._samples = samples\n",
    "        self._chunk_len = int(chunk_len_in_secs * sample_rate)\n",
    "        self._start = 0\n",
    "        self.output = True\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if not self.output:\n",
    "            raise StopIteration\n",
    "        last = self._start + self._chunk_len\n",
    "        if last <= len(self._samples):\n",
    "            chunk = self._samples[self._start: last]\n",
    "            start_time = self._start / self.sample_rate\n",
    "            self._start = last\n",
    "        else:\n",
    "            chunk = np.zeros(self._chunk_len, dtype='float32')\n",
    "            samp_len = len(self._samples) - self._start\n",
    "            chunk[:samp_len] = self._samples[self._start:len(self._samples)]\n",
    "            start_time = self._start / self.sample_rate\n",
    "            self.output = False\n",
    "        return chunk, start_time\n",
    "\n",
    "# A helper function for extracting samples as a numpy array from the audio file\n",
    "def get_samples(audio_file, target_sr=16000):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        sample_rate = f.samplerate\n",
    "        samples = f.read(dtype='float32')\n",
    "        if sample_rate != target_sr:\n",
    "            samples = librosa.resample(samples.T, orig_sr=sample_rate, target_sr=target_sr).T\n",
    "        return samples.flatten()\n",
    "\n",
    "# Function to prepare buffers and their start times\n",
    "def prepare_buffers(samples, config):\n",
    "    sample_rate = config['sample_rate']\n",
    "    buffer_len_in_secs = config['buffer_len_in_secs']\n",
    "    chunk_len_in_secs = config['chunk_len_in_secs']\n",
    "    n_buffers = config['n_buffers']\n",
    "\n",
    "    buffer_len = sample_rate * buffer_len_in_secs\n",
    "    sampbuffer = np.zeros(buffer_len, dtype=np.float32)\n",
    "    \n",
    "    chunk_reader = AudioChunkIterator(samples, chunk_len_in_secs, sample_rate)\n",
    "    chunk_len = sample_rate * chunk_len_in_secs\n",
    "    buffer_list = []\n",
    "    buffer_start_times = []\n",
    "    \n",
    "    for count, (chunk, start_time) in enumerate(chunk_reader, start=1):\n",
    "        sampbuffer[:-chunk_len] = sampbuffer[chunk_len:] \n",
    "        sampbuffer[-chunk_len:] = chunk\n",
    "        buffer_list.append(np.array(sampbuffer))\n",
    "        buffer_start_times.append(start_time)\n",
    "    return buffer_list, buffer_start_times\n",
    "\n",
    "def speech_collate_fn(batch):\n",
    "    _, audio_lengths = zip(*batch)\n",
    "    max_audio_len = max(audio_lengths).item()\n",
    "   \n",
    "    audio_signal= []\n",
    "    for sig, sig_len in batch:\n",
    "        sig_len = sig_len.item()\n",
    "        if sig_len < max_audio_len:\n",
    "            pad = (0, max_audio_len - sig_len)\n",
    "            sig = torch.nn.functional.pad(sig, pad)\n",
    "        audio_signal.append(sig)\n",
    "        \n",
    "    audio_signal = torch.stack(audio_signal)\n",
    "    audio_lengths = torch.stack(audio_lengths)\n",
    "    return audio_signal, audio_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioBuffersDataLayer(IterableDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._buf_count == len(self.signal):\n",
    "            raise StopIteration\n",
    "        self._buf_count += 1\n",
    "        return torch.as_tensor(self.signal[self._buf_count-1], dtype=torch.float32), \\\n",
    "               torch.as_tensor(self.signal_shape[0], dtype=torch.int64)\n",
    "        \n",
    "    def set_signal(self, signals):\n",
    "        self.signal = signals\n",
    "        self.signal_shape = self.signal[0].shape\n",
    "        self._buf_count = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "class ChunkBufferDecoder:\n",
    "    def __init__(self, ort_session, tokenizer, featurizer, stride, config):\n",
    "        self.ort_session = ort_session\n",
    "        self.input_names = [input.name for input in ort_session.get_inputs()]\n",
    "        self.output_names = [output.name for output in ort_session.get_outputs()]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.featurizer = featurizer\n",
    "        self.data_layer = AudioBuffersDataLayer()\n",
    "        self.data_loader = DataLoader(self.data_layer, batch_size=1, collate_fn=speech_collate_fn)\n",
    "        self.buffers = []\n",
    "        self.all_preds = []\n",
    "        self.chunk_len = config['chunk_len_in_secs']\n",
    "        self.buffer_len = config['buffer_len_in_secs']\n",
    "        assert config['chunk_len_in_secs'] <= config['buffer_len_in_secs']\n",
    "        \n",
    "        feature_stride = 0.01 # window_stride\n",
    "        self.model_stride_in_secs = feature_stride * stride\n",
    "        self.n_tokens_per_chunk = math.ceil(self.chunk_len / self.model_stride_in_secs) -2\n",
    "        self.blank_id = 1024\n",
    "        self.plot = False\n",
    "        \n",
    "    @torch.no_grad()    \n",
    "    def transcribe_buffers(self, buffers, buffer_start_times, merge=True, plot=False):\n",
    "        self.plot = plot\n",
    "        self.buffers = buffers\n",
    "        self.buffer_start_times = buffer_start_times\n",
    "        self.data_layer.set_signal(buffers[:])\n",
    "        self._get_batch_preds()\n",
    "        hyp, timestamps = self.decode_final(merge)    \n",
    "        return hyp, timestamps\n",
    "    \n",
    "    def _get_batch_preds(self):\n",
    "        device = 'cpu'\n",
    "        for batch in iter(self.data_loader):\n",
    "            audio_signal, audio_signal_len = batch\n",
    "            audio_signal, audio_signal_len = audio_signal.to(device), audio_signal_len.to(device)\n",
    "            features, features_length = self.featurizer.forward(audio_signal, audio_signal_len)\n",
    "            input_data = {\n",
    "                self.input_names[0]: features.cpu().numpy(),\n",
    "                self.input_names[1]: features_length.cpu().numpy()\n",
    "            }\n",
    "            log_probs = self.ort_session.run([self.output_names[0]], input_data)\n",
    "            greedy_predictions = torch.tensor(log_probs[0]).argmax(dim=-1, keepdim=False)\n",
    "            preds = torch.unbind(greedy_predictions)\n",
    "            for pred in preds:\n",
    "                self.all_preds.append(pred.cpu().numpy())\n",
    "\n",
    "    \n",
    "    def decode_final(self, merge=True):\n",
    "        self.unmerged = []\n",
    "        self.toks_unmerged = []\n",
    "        delay = math.ceil((self.chunk_len + (self.buffer_len - self.chunk_len) / 2) / self.model_stride_in_secs)\n",
    "\n",
    "        decoded_frames = []\n",
    "        all_toks = []\n",
    "        for idx, pred in enumerate(self.all_preds):\n",
    "            ids, toks, offsets = self._greedy_decoder(pred, self.tokenizer, self.buffer_start_times[idx])\n",
    "            decoded_frames.append((ids, offsets))\n",
    "            all_toks.append(toks)\n",
    "\n",
    "        for decoded, offsets in decoded_frames:\n",
    "            self.unmerged += list(zip(decoded[len(decoded) - 1 - delay:len(decoded) - 1 - delay + self.n_tokens_per_chunk],\n",
    "                                      offsets[len(offsets) - 1 - delay:len(offsets) - 1 - delay + self.n_tokens_per_chunk]))\n",
    "\n",
    "        if self.plot:\n",
    "            for i, tok in enumerate(all_toks):\n",
    "                plt.plot(self.buffers[i])\n",
    "                plt.show()\n",
    "                print(tok)\n",
    "                print(\"\\nGreedy labels collected from this buffer\")\n",
    "                print(tok[len(tok) - 1 - delay:len(tok) - 1 - delay + self.n_tokens_per_chunk])\n",
    "                self.toks_unmerged += tok[len(tok) - 1 - delay:len(tok) - 1 - delay + self.n_tokens_per_chunk]\n",
    "            print(\"\\nTokens collected from successive buffers before CTC merge\")\n",
    "            print(self.toks_unmerged)\n",
    "\n",
    "        if not merge:\n",
    "            return self.unmerged\n",
    "        hyp, timestamps = self.greedy_merge(self.unmerged)\n",
    "        return hyp, timestamps\n",
    "    \n",
    "    def _greedy_decoder(self, preds, tokenizer, buffer_start_time):\n",
    "        s = []\n",
    "        ids = []\n",
    "        offsets = []\n",
    "        for i in range(preds.shape[0]):\n",
    "            if preds[i] == self.blank_id:\n",
    "                s.append(\"_\")\n",
    "            else:\n",
    "                pred = preds[i]\n",
    "                s.append(tokenizer.id_to_piece(pred.item()))\n",
    "            ids.append(preds[i])\n",
    "            offsets.append(buffer_start_time + i * self.model_stride_in_secs)\n",
    "        return ids, s, offsets\n",
    "         \n",
    "    def greedy_merge(self, preds):\n",
    "        decoded_prediction = []\n",
    "        word_timestamps = []\n",
    "        previous = self.blank_id\n",
    "        for p, offset in preds:\n",
    "            if (p != previous or previous == self.blank_id) and p != self.blank_id:\n",
    "                decoded_prediction.append((p.item(), offset))\n",
    "            previous = p\n",
    "        hypothesis = self.tokenizer.decode_ids([p for p, _ in decoded_prediction])\n",
    "        \n",
    "        # Print word-level start and end timestamps\n",
    "        words = []\n",
    "        timestamps = []\n",
    "        current_word = \"\"\n",
    "        current_start_timestamp = 0.0\n",
    "        current_end_timestamp = 0.0\n",
    "        for token, timestamp in decoded_prediction:\n",
    "            subword = self.tokenizer.id_to_piece(token)\n",
    "            if subword.startswith(\"▁\"):\n",
    "                if current_word:\n",
    "                    words.append(current_word)\n",
    "                    timestamps.append((current_start_timestamp, current_end_timestamp + self.model_stride_in_secs))\n",
    "                current_word = subword.replace(\"▁\", \"\")\n",
    "                current_start_timestamp = timestamp\n",
    "            else:\n",
    "                current_word += subword\n",
    "            current_end_timestamp = timestamp\n",
    "            # print(current_word)\n",
    "        if current_word:\n",
    "            words.append(current_word)\n",
    "            timestamps.append((current_start_timestamp, current_end_timestamp + self.model_stride_in_secs))\n",
    "\n",
    "        for word, (start, end) in zip(words, timestamps):\n",
    "            word_timestamps.append({'word':word, 'start': start, 'end': end})\n",
    "            #print(f\"{word}: {start:.2f}s - {end:.2f}s\")\n",
    "        \n",
    "        return hypothesis, word_timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"chunk_len_in_secs\": 4,\n",
    "    \"context_len_in_secs\": 2,\n",
    "    \"buffer_len_in_secs\": 6,\n",
    "    \"n_buffers\": 5,\n",
    "    \"stride\": 4\n",
    "}\n",
    "\n",
    "\n",
    "file_path='/home/svanga/PromptingNemo/applications/voicebot/a.wav'\n",
    "output_mono_path='/home/svanga/PromptingNemo/applications/voicebot/b.wav'\n",
    "mono_audio_path = convert_to_mono(file_path, output_mono_path, 0)\n",
    "config['n_buffers'] = calculate_n_buffers(mono_audio_path, config[\"chunk_len_in_secs\"], config[\"buffer_len_in_secs\"], config[\"stride\"])\n",
    "samples = get_samples(mono_audio_path, target_sr=config[\"sample_rate\"])\n",
    "buffer_list, buffer_start_times = prepare_buffers(samples, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spm_model_path:  /home/svanga/test/model_shelf/EN_IoT_conformer_ctc_large/tokenizer/tokenizer.model\n",
      "Transcription: other the reformorms that  took place around this time included the organisation of units into standdardd formations such as battlealions END.  increased payments to volulunteerersEND landand rants for afffien serviceEND. the establishment ofENTITY-EVENT_NAME anual  train  cabs END usually over ENTITY-EVENT_NAME easter END.  thecre of cris of professional soders known as permaninent staffff END. to provide  training the requirement for officeerss at non commissioned officeiceers . to  pass exams at the esttablishment of minimum required attendance. INTENT-CALENDAR_SET \n",
      "[{'word': 'other', 'start': 0.0, 'end': 2.08}, {'word': 'the', 'start': 2.12, 'end': 2.16}, {'word': 'reformorms', 'start': 2.2, 'end': 2.68}, {'word': 'that', 'start': 2.84, 'end': 2.88}, {'word': 'took', 'start': 3.04, 'end': 3.2}, {'word': 'place', 'start': 3.2800000000000002, 'end': 3.3200000000000003}, {'word': 'around', 'start': 3.6, 'end': 3.64}, {'word': 'this', 'start': 3.96, 'end': 4.0}, {'word': 'time', 'start': 4.12, 'end': 4.16}, {'word': 'included', 'start': 4.44, 'end': 4.76}, {'word': 'the', 'start': 4.8, 'end': 4.84}, {'word': 'organisation', 'start': 4.88, 'end': 5.5200000000000005}, {'word': 'of', 'start': 5.640000000000001, 'end': 5.680000000000001}, {'word': 'units', 'start': 5.8, 'end': 6.08}, {'word': 'into', 'start': 6.2, 'end': 6.36}, {'word': 'standdardd', 'start': 6.5600000000000005, 'end': 6.840000000000001}, {'word': 'formations', 'start': 7.04, 'end': 7.44}, {'word': 'such', 'start': 7.6, 'end': 7.840000000000001}, {'word': 'as', 'start': 7.88, 'end': 7.96}, {'word': 'battlealions', 'start': 8.0, 'end': 8.559999999999999}, {'word': 'END.', 'start': 8.600000000000001, 'end': 8.719999999999999}, {'word': 'increased', 'start': 9.08, 'end': 9.479999999999999}, {'word': 'payments', 'start': 9.64, 'end': 9.879999999999999}, {'word': 'to', 'start': 10.04, 'end': 10.079999999999998}, {'word': 'volulunteerersEND', 'start': 10.16, 'end': 10.879999999999999}, {'word': 'landand', 'start': 11.04, 'end': 11.32}, {'word': 'rants', 'start': 11.4, 'end': 11.799999999999999}, {'word': 'for', 'start': 11.92, 'end': 11.959999999999999}, {'word': 'afffien', 'start': 12.0, 'end': 12.48}, {'word': 'serviceEND.', 'start': 12.600000000000001, 'end': 13.079999999999998}, {'word': 'the', 'start': 13.48, 'end': 13.52}, {'word': 'establishment', 'start': 13.56, 'end': 14.239999999999998}, {'word': 'ofENTITY-EVENT_NAME', 'start': 14.44, 'end': 14.6}, {'word': 'anual', 'start': 14.6, 'end': 14.84}, {'word': 'train', 'start': 15.04, 'end': 15.079999999999998}, {'word': 'cabs', 'start': 15.44, 'end': 15.6}, {'word': 'END', 'start': 15.64, 'end': 15.76}, {'word': 'usually', 'start': 15.84, 'end': 16.28}, {'word': 'over', 'start': 16.44, 'end': 16.48}, {'word': 'ENTITY-EVENT_NAME', 'start': 16.64, 'end': 16.759999999999998}, {'word': 'easter', 'start': 16.8, 'end': 16.84}, {'word': 'END.', 'start': 17.04, 'end': 17.2}, {'word': 'thecre', 'start': 17.44, 'end': 17.72}, {'word': 'of', 'start': 18.0, 'end': 18.04}, {'word': 'cris', 'start': 18.04, 'end': 18.599999999999998}, {'word': 'of', 'start': 18.72, 'end': 18.759999999999998}, {'word': 'professional', 'start': 18.92, 'end': 19.36}, {'word': 'soders', 'start': 19.6, 'end': 19.96}, {'word': 'known', 'start': 20.36, 'end': 20.52}, {'word': 'as', 'start': 20.6, 'end': 20.68}, {'word': 'permaninent', 'start': 20.8, 'end': 21.36}, {'word': 'staffff', 'start': 21.44, 'end': 21.8}, {'word': 'END.', 'start': 21.8, 'end': 21.96}, {'word': 'to', 'start': 22.28, 'end': 22.32}, {'word': 'provide', 'start': 22.48, 'end': 22.52}, {'word': 'training', 'start': 22.92, 'end': 23.2}, {'word': 'the', 'start': 23.44, 'end': 23.48}, {'word': 'requirement', 'start': 23.52, 'end': 24.0}, {'word': 'for', 'start': 24.240000000000002, 'end': 24.28}, {'word': 'officeerss', 'start': 24.52, 'end': 24.88}, {'word': 'at', 'start': 25.04, 'end': 25.08}, {'word': 'non', 'start': 25.16, 'end': 25.4}, {'word': 'commissioned', 'start': 25.48, 'end': 25.96}, {'word': 'officeiceers', 'start': 26.08, 'end': 26.36}, {'word': '.', 'start': 26.4, 'end': 26.52}, {'word': 'to', 'start': 26.68, 'end': 26.72}, {'word': 'pass', 'start': 26.88, 'end': 27.08}, {'word': 'exams', 'start': 27.12, 'end': 27.52}, {'word': 'at', 'start': 27.68, 'end': 27.72}, {'word': 'the', 'start': 27.88, 'end': 27.919999999999998}, {'word': 'esttablishment', 'start': 27.92, 'end': 28.56}, {'word': 'of', 'start': 28.72, 'end': 28.759999999999998}, {'word': 'minimum', 'start': 28.84, 'end': 29.16}, {'word': 'required', 'start': 29.32, 'end': 29.72}, {'word': 'attendance.', 'start': 29.84, 'end': 30.36}, {'word': 'INTENT-CALENDAR_SET', 'start': 30.36, 'end': 30.439999999999998}]\n"
     ]
    }
   ],
   "source": [
    "ort_session_en_ner, model_tokenizer_en, filterbank_featurizer = create_ort_session(model_name=\"EN_IoT_conformer_ctc_large\", model_shelf=\"/home/svanga/test/model_shelf\")\n",
    "\n",
    "stride = 4 # 4, 8 for Citrinet\n",
    "asr_decoder = ChunkBufferDecoder(ort_session_en_ner, model_tokenizer_en, filterbank_featurizer, stride, config)\n",
    "transcription, timestamps = asr_decoder.transcribe_buffers(buffer_list, buffer_start_times, plot=False)\n",
    "print(\"Transcription:\", transcription)\n",
    "print(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spm_model_path:  /home/svanga/test/model_shelf/EN_ner_conformer_ctc_large/tokenizer/tokenizer.model\n",
      "Transcription: ther reforms that took place around this time included the organization of units into standard formations such as battalions increased payments to volunteers , land grants for efficient service , the establishment of NER_DATE annual END training camps , usually over  Easter END , the creation of cadderies of professional soldiers known as permanent staff to provide training , the requirement for officers and non-commissioned officers to pass exams and the establishment of minimum required attendance\n",
      "[{'word': 'ther', 'start': 0.0, 'end': 2.0}, {'word': 'reforms', 'start': 2.16, 'end': 2.64}, {'word': 'that', 'start': 2.8000000000000003, 'end': 2.8800000000000003}, {'word': 'took', 'start': 2.96, 'end': 3.12}, {'word': 'place', 'start': 3.2, 'end': 3.3600000000000003}, {'word': 'around', 'start': 3.52, 'end': 3.7600000000000002}, {'word': 'this', 'start': 3.84, 'end': 3.92}, {'word': 'time', 'start': 4.08, 'end': 4.16}, {'word': 'included', 'start': 4.4, 'end': 4.640000000000001}, {'word': 'the', 'start': 4.72, 'end': 4.8}, {'word': 'organization', 'start': 5.12, 'end': 5.6}, {'word': 'of', 'start': 5.6, 'end': 5.68}, {'word': 'units', 'start': 5.76, 'end': 6.0}, {'word': 'into', 'start': 6.24, 'end': 6.32}, {'word': 'standard', 'start': 6.5600000000000005, 'end': 6.800000000000001}, {'word': 'formations', 'start': 7.04, 'end': 7.36}, {'word': 'such', 'start': 7.6, 'end': 7.76}, {'word': 'as', 'start': 7.76, 'end': 7.84}, {'word': 'battalions', 'start': 8.0, 'end': 8.48}, {'word': 'increased', 'start': 9.04, 'end': 9.36}, {'word': 'payments', 'start': 9.52, 'end': 9.84}, {'word': 'to', 'start': 10.0, 'end': 10.08}, {'word': 'volunteers', 'start': 10.16, 'end': 10.8}, {'word': ',', 'start': 10.879999999999999, 'end': 10.959999999999999}, {'word': 'land', 'start': 11.04, 'end': 11.200000000000001}, {'word': 'grants', 'start': 11.44, 'end': 11.76}, {'word': 'for', 'start': 11.92, 'end': 12.0}, {'word': 'efficient', 'start': 12.08, 'end': 12.56}, {'word': 'service', 'start': 12.64, 'end': 13.040000000000001}, {'word': ',', 'start': 13.120000000000001, 'end': 13.200000000000001}, {'word': 'the', 'start': 13.44, 'end': 13.52}, {'word': 'establishment', 'start': 13.6, 'end': 14.24}, {'word': 'of', 'start': 14.24, 'end': 14.32}, {'word': 'NER_DATE', 'start': 14.32, 'end': 14.48}, {'word': 'annual', 'start': 14.48, 'end': 14.8}, {'word': 'END', 'start': 14.8, 'end': 14.959999999999999}, {'word': 'training', 'start': 15.04, 'end': 15.360000000000001}, {'word': 'camps', 'start': 15.44, 'end': 15.68}, {'word': ',', 'start': 15.76, 'end': 15.84}, {'word': 'usually', 'start': 15.92, 'end': 16.08}, {'word': 'over', 'start': 16.240000000000002, 'end': 16.32}, {'word': 'Easter', 'start': 16.64, 'end': 17.119999999999997}, {'word': 'END', 'start': 17.12, 'end': 17.279999999999998}, {'word': ',', 'start': 17.36, 'end': 17.439999999999998}, {'word': 'the', 'start': 17.44, 'end': 17.52}, {'word': 'creation', 'start': 17.6, 'end': 17.84}, {'word': 'of', 'start': 17.84, 'end': 17.919999999999998}, {'word': 'cadderies', 'start': 18.08, 'end': 18.639999999999997}, {'word': 'of', 'start': 18.72, 'end': 18.799999999999997}, {'word': 'professional', 'start': 18.88, 'end': 19.439999999999998}, {'word': 'soldiers', 'start': 19.52, 'end': 20.0}, {'word': 'known', 'start': 20.240000000000002, 'end': 20.32}, {'word': 'as', 'start': 20.48, 'end': 20.56}, {'word': 'permanent', 'start': 20.96, 'end': 21.36}, {'word': 'staff', 'start': 21.52, 'end': 21.84}, {'word': 'to', 'start': 22.16, 'end': 22.24}, {'word': 'provide', 'start': 22.4, 'end': 22.72}, {'word': 'training', 'start': 22.8, 'end': 23.119999999999997}, {'word': ',', 'start': 23.2, 'end': 23.279999999999998}, {'word': 'the', 'start': 23.36, 'end': 23.439999999999998}, {'word': 'requirement', 'start': 23.52, 'end': 24.0}, {'word': 'for', 'start': 24.08, 'end': 24.159999999999997}, {'word': 'officers', 'start': 24.32, 'end': 24.72}, {'word': 'and', 'start': 25.04, 'end': 25.119999999999997}, {'word': 'non-commissioned', 'start': 25.2, 'end': 26.0}, {'word': 'officers', 'start': 26.16, 'end': 26.479999999999997}, {'word': 'to', 'start': 26.64, 'end': 26.72}, {'word': 'pass', 'start': 26.8, 'end': 26.959999999999997}, {'word': 'exams', 'start': 27.12, 'end': 27.52}, {'word': 'and', 'start': 27.68, 'end': 27.759999999999998}, {'word': 'the', 'start': 27.76, 'end': 27.84}, {'word': 'establishment', 'start': 27.92, 'end': 28.479999999999997}, {'word': 'of', 'start': 28.560000000000002, 'end': 28.64}, {'word': 'minimum', 'start': 28.72, 'end': 29.279999999999998}, {'word': 'required', 'start': 29.44, 'end': 29.759999999999998}, {'word': 'attendance', 'start': 29.92, 'end': 30.32}]\n"
     ]
    }
   ],
   "source": [
    "ort_session_en_ner, model_tokenizer_en, filterbank_featurizer = create_ort_session(model_name=\"EN_ner_conformer_ctc_large\", model_shelf=\"/home/svanga/test/model_shelf\")\n",
    "stride = 8 # 4, 8 for Citrinet\n",
    "asr_decoder = ChunkBufferDecoder(ort_session_en_ner, model_tokenizer_en, filterbank_featurizer, stride, config)\n",
    "transcription, timestamps = asr_decoder.transcribe_buffers(buffer_list, buffer_start_times, plot=False)\n",
    "print(\"Transcription:\", transcription)\n",
    "print(timestamps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
